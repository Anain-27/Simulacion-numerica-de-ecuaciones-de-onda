% !TeX encoding = ISO-8859-1
\chapter{EXPERIMENTOS COMPUTACIONALES}
\label{cha:arbolesclasificacion}

En esta sección se analizarán los métodos explicados en la Sección \ref{cha:svmmulticlase} estos son, OVO, OVA, DAGSVM, Modelo Weston-Watkins y Modelo Crammer-Singer, el objetivo es comparar los resultados obtenidos al aplicar dichos métodos. Los experimentos que se llevarán a cabo en esta sección han sido realizado en el entorno Jupyter usando el lenguaje de programación Python. \\

Para los modelos OVO, OVA y Crammer-Singer se han utilizado librerías del paquete sklearn que los resuelven mientras que por otro lado para los modelos DAGSVM y Weston-Watkins se ha tenido que desarrollar el código para resolverlos, puesto que no existen librerías como en los casos anteriores que los resuelvan.


\section{CONJUNTOS DE ENTRENAMIENTO}

Para realizar los experimentos computacionales se han usado 3 bases de datos las cuales se pueden encontrar en el repositorio UCI, estas bases de datos contienen una pequeña cantidad de atributos debido al incremento en tiempo computacional que supondría hacer los cálculos con más atributos. A continuación se explicará brevemente las bases de datos que se utilizarán.\\

\begin{itemize}
	\item IRIS: Es una de las bases más usadas en reconocimiento de patrones. Esta base de datos posee 150 plantas de género Iris, en concreto a lo largo de las 150 observaciones nos encontramos con 3 clases distintas, Iris Setosa, Iris Versicolour e Iris Virginica, cada una de ellas con 50 de las 150 observaciones de la base de datos. Posee 4 atributos que son longitud y anchura del sépalo y del pétalo.  
	\item Contraceptive Method Choice (CMC): En este base de datos se posee información de 1473 mujeres casadas, en concreto se han estudiado 10 características de estas mujeres como la edad, nº de hijos que ha tenido, su educación, la educación de su pareja... La variable que determina la clase es que método anticonceptivo usan, existen tres posibilidades, que no usen, un método de larga duración o un método de corta duración.
	\item Car Evolution Database (CED): Esta base de datos dispone de información sobre 1728 coches, en concreto el estudio provee 6 atributos para cada coche como número de puertas, precio de compra, capacidad del vehículo..., y así clasifica los coches en 4 clases diferentes. %https://archive.ics.uci.edu/ml/datasets/Car+Evaluation
\end{itemize}
	%Wine (W): Esta base de datos dispone de información sobre 178 análisis a vinos que han crecido en la misma zona de Italia, pero provienen de 3 cultivos diferentes. Como resultado de cada análisis conocemos 13 atributos de cada vino que se ha estudiado.

La tabla de abajo muestra un pequeño resumen de las propiedades que conocemos de las bases de datos, la variable \textbf{m} corresponde con el número de datos, \textbf{d} con la cantidad de atributos y \textbf{n} el número de clases.

\begin{table}[H]
	\begin{center}
		\begin{tabular}{ | m{2.5cm} | m{2.5cm} | m{2.5cm} | m{2.5cm} | }
			\hline {\centering}Bases de datos &  \centering \textbf{m} & \centering \textbf{d} & \hspace{11mm}\textbf{n} \\ \hline
		    \centering IRIS & \centering$150$ & \centering $4$ &\hspace{10mm}  3 \\ \hline
			\centering CMC &\centering $1473$ & \centering $10$ &\hspace{10mm}  3 \\ \hline
			\centering CED & \centering{$1728$}&\centering $6$  &\hspace{10mm}  4 \\ \hline
		\end{tabular}
	\end{center}
\end{table}

\section{RESULTADOS PREVIOS}
\textbf{COMENTARIO: Quiero escribir la tarjeta que estoy usando para hacer las operaciones en python.}.\\

Para valorar la eficiencia de cada método, se utilizarán los valores de ACC, el parámetro C que da mayor precisión a cada método y el tiempo que tarda el programa en realizar el entrenamiento del modelo y la obtención de estos valores. A continuación explicaremos con más detalle los dos primeros valores que se han expuesto.
\begin{itemize}
	%\item AUC: Este indicador se puede interpretar como la probabilidad  de que el clasificador pueda distinguir entre las diferentes clases. Su fórmula es la siguiente 
	%\begin{equation}\nonumber
	%AUC =\frac{\frac{VP}{VP+FN}+\frac{VN}{VN+FP}}{2}
	%\end{equation}
	\item ACC: Este valor nos indica la calidad del modelo, esto es, el porcentaje de elementos clasificados correctamente. 
	\begin{equation}\nonumber
	ACC =\frac{VP+VN}{VP+VN+FP+FN}
	\end{equation} 
	siendo VP,VN,FP y FNv denonimados como verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos respectivamente y corresponden a los valores que se encuentran en la matriz de confusión.\\
	
	%esta página para tablas y tal está to bien https://www.ucm.es/data/cont/docs/1346-2019-04-12-BaSix%20LaTeX%20ba%CC%81sico%20con%20ejercicios%20resueltos%20-%20Noir16.pdf
	
	
	\includegraphics[width=0.8\textwidth]{matrizconfusion.eps}
	\item Parámetro C: Con este parámetros se penaliza el uso de las variable de holgura $\boldsymbol{\xi}$. Para la obtención del parámetro C utilizaremos la técnica de la validación cruzada (Cross Validation), para este proceso se busca dividir los datos en T pliegues o conjuntos, de estos T pliegues se utilizan (T-1) para el entrenamiento del modelo y el restante para la fase de testeo, el algoritmo en cuestión se entrenará y probará T veces, cada vez que se utiliza un nuevo conjunto como conjunto de prueba. Los parámetros que se probarán de C son los siguientes $$C=\{2^{-4},2^{-3},2^{2},2^{-1},1,2,2^{2},2^{3},2^{4}\}$$ \\
	
\end{itemize}

\textbf{COMENTARIO: Tengo que añadir las tablas con los resultados, pero estoy programando los métodos DAGSVM y WW, que no vienen en librerías de python, y la división en grupos de 5 con una proporción de clases parecida en cada grupo.}

%https://www.iartificial.net/precision-recall-f1-accuracy-en-clasificacion/  \quad (HAY OTRA PAGINA ESCRITA EN COMENTARIO, PORQUE ME DA UN ERROR AL ESCRIBIRLA TAL CUAL)\\
%https://sitiobigdata.com/2019/01/19/machine-learning-metrica-clasificacion-parte-3/# 
%ACCURACY: 	porcentaje total de elementos clasificados correctamente. $$\frac{TP+TN}{TP+TN+FP+FN}$$
%Es un valor entre 0 y 1. Cuanto más alto, mejor. No funciona bien cuando las clases están desbalanceadas, pero creo que yo si lo voy a balancear.\\
%RECALL/TASA DE A TP/SENSIBILIDAD: Es el número de elementos identificados correctamente como positivos del total de positivos verdaderos.$$\frac{TP}{TP+FN}$$\\
%PRECISION: número de elementos identificados correctamente como positivo de un total de elementos identificados como positivos $$\frac{TP}{TP+FP}$$\\

%Precisión trata de ser preciso. Entonces, incluso si logramos capturar solo un caso de cáncer y lo capturamos correctamente, entonces somos 100\% precisos.\\

%Recall no se trata tanto de capturar casos correctamente sino más de capturar todos los casos que tener ?cáncer? con la respuesta como ?cáncer?. Entonces,
%si siempre decimos cada caso como ?cáncer?, tenemos un recuerdo del 100\%.\\

%Entonces, básicamente, si queremos enfocarnos más en minimizar los falsos negativos, deseamos que nuestro recuerdo sea lo más cercano posible al 100\% sin la precisión es muy mala y si queremos enfocarnos en minimizar los falsos positivos, entonces nuestro enfoque debe orientarse a hacer que la Precisión sea lo más cercana posible al 100\%.\\
%EXACTITUD\\

%AUC\\


%ÁRBOLES DE CLASIFICACIÓN

%Si la entropía es 0, entonces es no se sigue más por ese lado. Si la entropía es 1 entonces los datos están divididos equitativamente.\\

%Information gain (ganancia de información) el resultado es la característica con mejores resultados para separar los datos. A menores valores de entropía, mayor información se gana, y por el contrario a mayor valor de entropía menor información.\\

%El algoritmo ID3 está relacionado con la ganancia de información???. Osea creo que todo lo que veo es el algoritmo ID3.\\

%Un valor se puede parar, porque todos los nodos tengan entropía 0, porque se imponga una profunidad al árbol o porque se llegue a un número menor de valores deseados (no se si hay más posibilidades).\\

%La diferencia entre entropía y gini impurity es que en el primero se alcanza el máximo en 1 y en el segundo en 0.5, pero creo que el concepto en ambos es el mismo. Gini impurity es más sencillo de resolver, porque con la entropía tiene un log y el otro solo tiene un cuadrado.\\

% ----------------------------------------------------------------------

