% !TeX encoding = ISO-8859-1


\chapter{Preliminares}
\label{cha:preliminares}

\section{SVM}
\textbf{COMENTARIO: Supongo que en la introducción hablaré sobre el machine learning y sobre los tipos de aprendizaje que existen, incluidos el aprendizaje supervisado.}\\

Dentro del aprendizaje supervisado se encuentra un conjunto de algoritmos que se conocen como SVM (Support Vector Machine) que fueron realizados por Vladimir Vapnik y sus colaboradores (B.Boser,I.Guyon and V.Vapnik, 1992).\\
%Uno de los retos que la humandidad siempre ha querido alcanzar es replicar la inteligencia que caracteriza a nuestra especie, siendo un papel vital para esta hazaña el Machine Learning. Éste gracias a diversos algoritmos y datos aportados pretende imitar la forma de pensar del ser humano, siendo capaz de modificarse y desarrollarse por sí misma.\\

%La primera constancia que hay de un sistema de Machine Learning es en 1950 en el conocido Test de Turing, creado por Alan Turing, cuyo objetivo era comprobar si una máquina puede demostrar comportamiento inteligente. Dos años después en 1952 Arthur Samuel escribe el primer algoritmo para jugar a las damas, que a pesar de ser un juego relativamente sencillo, la dificultad reside en adaptarse a los movimientos que realiza el contrincante.\\

%El término Machine Learning nace en la conferencia de Darmouth en 1956, donde se asentarían las bases de lo que se conoce actualmente como Inteligencia Artificial.\\

%Dentro del Machine Learning nos encontramos diferentes formas de aprendizaje, Supervisado, No Supervisado y por Refuerzo. En el aprendizaje supervisado  los datos están etiquetados, esto es, se conoce el valor del atributo objetivo permitiendo así que el algoritmo desarrolle una función que prediga dicho atributo para datos nuevos en los que se desconozca. Por otro lado el aprendizaje no supervisado no parte de datos etiquetado, en vez de predecir valores se usan para agrupar datos y buscar grupos similares entre los datos proporcionados. Por último el aprendizaje por refuerzo se automodifica a partir de experimentar con los datos, tomará decisiones en función de recompensas y penalizaciones que haya recibido por elecciones tomadas.\\

El SVM parte de un conjunto de datos que se expresan en forma de dupla como $(\textbf{x}_{i},y_{i})$ para $i=1,\dots,n$, con $\textbf{x}_{i}\in\Omega\subset\mathbb{R}^{m}$ e $y_{i}$ perteneciente a un conjunto formado por dos elementos Y=\{-1,1\}.El conjunto de datos se puede dividir en dos subconjuntos uno que verifique que $y=-1$ y otro $y=1$, dichos subgrupos se denominan clases, siendo el objetivo del SVM hallar un hiperplano que las separe.\\

Al principio solo se estudió el caso en el cual el hiperplano de separación fuera lineal. Posteriormente mediante el uso de la función kernel, la cual definiremos en la subsección \ref{LNS}, se podrán resolver los casos en los que el hiperplano de separación no sea lineal (C.Cortes and V.Vapnik, 1995).\\
%corine Cortes, Vapnik  Vladimir y por otro lado el señor con el nomre raro berhand y smola.


%@ARTICLE Cortes, C., Vapnik, V. Support-vector networks. Mach Learn 20, 273?297 (1995). https://doi.org/10.1007/BF00994018

Las aplicaciones del SVM son diversas, en general es usado para problemas de clasificación y regresión entre otros. Un ejemplo podría ser, un conjunto de individuos con diferentes atributos y algunos tiene cáncer y otros no, lo que se busca es separar a los individuos que padecen cáncer de los que no lo tienen, en este caso las clases serían $y=1$ si el individuo padece cáncer e $y=-1$ al caso contrario.\\

El SVM tiene diferentes formulaciones en función de la relación que tengan los elementos de $\Omega$ entre sí. Dichas relaciones se pueden clasificar en dos:

\begin{itemize}
	\item Separables linealmente: Si se puede definir un hiperplano de separación que permite separar ambas clases entre sí.
	\item No separables linealmente: Si no existe hiperplano de separación que permita separar una clase de la otra. 
\end{itemize}
% no se donde exactamente poner los cuasi-separables
Veamos como serían las formulaciones para cada uno de los casos mencionados anteriormente.
\subsection{LINEALMENTE SEPARABLE}

En este caso, sí existe un hiperplano de separación entre ambas clases, definimos este hiperplano como
\begin{equation}\label{hiperplano}
H(\textbf{x})=(v_{1}x_{1}+...+v_{n}x_{n}) + a= <\textbf{v},\textbf{x}> + a,
\end{equation}
donde $a\in \mathbb{R}$ es una constante, $\textbf{v}\in\mathbb{R}^m$ es el vector normal que define el hiperplano y $<\cdot, \cdot>$ es el producto escalar entre dos vectores.\\

Una forma de separar dichas clases es imponiendo que el hiperplano verifique las dos siguientes expresiones

\begin{equation*}
<\textbf{v},\textbf{x}_{i}> + \hspace{2mm} a \geq 0, \text{ si } y=1 \text{ para }  i=1,\dots,n,
\end{equation*}

\begin{equation*}
<\textbf{v},\textbf{x}_{i}> + \hspace{2mm} a \leq 0, \text{ si } y=-1, \text{ para } i=1,\dots,n,
\end{equation*}
o lo que es lo mismo
\begin{equation}\label{hiperplano2}\nonumber
y_{i}(<\textbf{v},\textbf{x}_{i}>+a)\geq 0,   \qquad \text{para } i=1,\dots ,n,
\end{equation}

\begin{figure}[h]	
	\centering
	\subfloat[]{
		
		\includegraphics[width=0.5\textwidth]{datoshiperplanoseparables}
		\label{figura311}	
	}
	\hspace*{3mm}
	\subfloat[]{
		
		\includegraphics[width=0.45\textwidth]{margen}	
		\label{figura312}
	}
	\caption{En la figura \ref{figura311} se puede observar un hiperplano lineal, que separa dos clases de datos. Por otro lado la figura \ref{figura312} corresponde con una representación gráfica del concepto de margen dado en la definición \ref{definicionmargen}}
\end{figure}

A continuación se define el concepto de margen, el cual se usará para formular el problema de hallar el hiperplano lineal que separe las clases.\\

\begin{definicion}[label={definicionmargen},nameref={Title or anything else}]{Margen:}
	\textit{Sea $\Omega$ un conjunto de datos y $H$ un hiperplano que separe dichos datos en dos conjuntos distintos.  Llamaremos margen ($\tau$) de un hiperplano $H$ a la distancia mínima que hay entre dicho hiperplano y el elemento más cercano de los dos conjuntos que separa.}	
\end{definicion}

\begin{wrapfigure}{r}{0.5\textwidth}
	\includegraphics[width=0.5\textwidth]{infinitoshiperplanos}	
	\label{figura32}	
	\caption{En la figura se representan múltiples hiperplanos lineales que separan dos clases}
\end{wrapfigure}
Hay infinitos hiperplanos que separen las dos clases de datos, pero buscamos aquel cuyo margen sea máximo, o lo que es lo mismo, que la distancia entre las clases sea máxima. Sabemos que la distancia de un punto $c$, de un hiperplano $H$, a dicho hiperplano viene dada por $\frac{\abs{H(c)}}{\norm{\textbf{v}}_{2}}$, siendo $\abs{\cdot}$ el valor absoluto y $\norm{\cdot}_{2}$ la norma-2, para facilitar la notación se denotará de ahora en adelante como $\norm{\cdot}$. \\

Por tanto, se busca que la distancia entre cualquier elemento de la dupla $(\textbf{x}_{i},y_{i})$ para $i=1,\dots,n$ y el hiperplano $H$ sea al menos la misma que el margen, o lo que es lo mismo 
\begin{equation}\label{margen}
\frac{y_{i}H(\textbf{x}_{i})}{\norm{\textbf{v}}}\geq \tau.
\end{equation}

De la ecuación \eqref{margen} se deduce que para que el margen sea máximo es necesario encontrar el vector $\textbf{v}$ más pequeño. Definimos el hiperplano óptimo como aquel que verifica que el margen sea máximo, y por tanto, que el vector $\textbf{v}$ sea el mínimo; Para que dicho hiperplano sea único imponemos que $\tau\norm{\textbf{v}}=1$. Se concluye entonces que el hiperplano óptimo viene dado por la siguiente expresión
\begin{equation}\label{hiperplanooptimo}
y_{i}(<\textbf{v},\textbf{x}_{i}>+a)\geq 1,   \qquad i=1,\dots ,n.
\end{equation}



\begin{definicion}[label={vectoresoporte},nameref={Title or anything else}]{Vectores Soporte}
	\textit{Sea $(\textbf{x}_{i},y_{i})$ con $\textbf{x}_{i}\in\mathbb{R}^{m}$ e $y_{i}\in\{1,-1\}$ tal que $i=1,\dots,n$. Existe un hiperplano lineal $H$ que separa la clase $y=1$ de $y=-1$. Llamaremos vectores soporte, a aquellos vectores que verifiquen la igualdad en la ecuación \eqref{hiperplanooptimo}.}
\end{definicion}

\begin{wrapfigure}{l}{0.5\textwidth}
	\includegraphics[width=0.5\textwidth]{vectorsoporte}	
	\label{figura32}	
	\caption{En la figura, los dos puntos rojos representan a los vectores soporte del hiperplano que separa las dos clases.}
\end{wrapfigure}
%\begin{figure}[h]
	%\centering
	%\includegraphics[width=0.5\textwidth]{datoscuasiseparablehiperplano}
	%\caption{Ejemplo de un hiperplano que separa ambas clases, pero no están perfectamente separadas.}	
%\end{figure}

%escribo caracterización convexidad de segundo orden y demuestro ¿pongo el teorema del valor medio, potgo la caracterizaciónde un orden?, por último demuestro que la convexidad de la función cuadrática, y  restricción?.

Por tanto, el problema para hallar el hiperplano óptimo es equivalente a buscar el valor mínimo de $\norm{\textbf{v}}$, sujeto a la expresión \eqref{hiperplanooptimo}. Para simplificar cálculos, se usará como función objetivo la expresión $\frac{1}{2}\norm{\textbf{v}}^{2}$ en vez de $\norm{\textbf{v}}$,  dado que es análogo minimizar una expresión que otra, luego tenemos el siguiente problema.
\begin{alignat*}{3}
\textbf{(P1)} \quad & \text{min}   \quad && \frac{1}{2}\norm{\textbf{v}}^{2} &&\\
& \text{s.a}   \quad && \eqref{hiperplanooptimo} \quad &&\\
& \quad && i=1,\dots,n
\end{alignat*}

Nos encontramos ante un problema con $n$ variables y $n$ restricciones, usaremos el método de multiplicadores de Lagrange pasando así a un problema sin restricciones y con $2n$ variables, correspondiendo $n$ de ellas a multiplicadores de Lagrange.\\

\begin{definicion}[label={definicionfuncionlagrange},nameref={Title or anything else}]{Función de Lagrange}
	\textit{Sea un problema de programación entera}
	\begin{alignat*}{3}
	 \textbf{(PP)} \quad & \text{min}   \quad && f(\textbf{x}) && \\
	& \text{s.a}   \quad && g_{i}(\textbf{x}) \leq 0 \quad && \\
	&\quad  && x\in \mathbb{R}^{m}, \quad i=1,\dots,n. &&
	\end{alignat*}
	
	\textit{su función de Lagrange viene dada por} 
	\begin{equation}\label{definicionlagrange}\nonumber
	L(\textbf{x},\boldsymbol{\alpha})=f(\textbf{x}) + \sum_{i=1}^{n}\alpha_{i}g_{i}(\textbf{x})
	\end{equation}
	
	\textit{donde $\alpha_{i}\geq0$ para $i=1,\dots,n$ son los multiplicadores de Lagrange. }
\end{definicion}



En este caso, la función de Lagrange sería la siguiente
\begin{equation}\label{lagrange}
L(\textbf{v},a,\boldsymbol{\alpha})=\frac{1}{2}\norm{\textbf{v}}^{2}-\sum_{i=1}^{n}\alpha_{i}(y_{i}(<\textbf{v},\textbf{x}_{i}>+a)-1)
\end{equation}
con $\alpha_{i}\geq 0$, el símbolo negativo de la ecuación se debe a que las restricciones en el problema \textbf{(P1)} son mayores o iguales que 0. Pasaremos a definir nuestro problema dual, para ello se verá primero lo que es un problema relajado.\\


\begin{definicion}[label={problemarelajado},nameref={Title or anything else}]{Problema Relajado}
	\textit{Sea} \textbf{(PP)}\textit{ un problema de programación entera, decimos que su problema relajado} \textbf{(PR)} \textit{ es el resultante de eliminar las condiciones de integridad en las variables de} \textbf{(PP)}.
\end{definicion}

Veamos que el problema \textbf{(PR)} es el problema relajado de \textbf{(PP)}, siendo \textbf{(PR)} el siguiente

\begin{alignat*}{3}
\textbf{(PR)} \quad & \text{min}   \quad && f(\textbf{x})+\sum_{i=1}^{n}\lambda_{i}g_{i}(\textbf{x}) && \\
&  \quad && x\in \mathbb{R}^{m}, \quad \lambda_{i}\geq 0, \quad i=1,\dots,n && \\
\end{alignat*}
%\begin{equation}
%\begin{split}
%& \text{máx}   \quad  \inf L(\textbf{v},a,\boldsymbol{\alpha})=\frac{1}{2}\norm{\textbf{v}}^{2}-\sum_{i=1}^{n}\alpha_{i}(y_{i}(<\textbf{v},\textbf{x}_{i}>+a)-1) \\
%& \alpha_{i}\geq 0 \quad  i=1,\dots,n\\
%\end{split}
%\end{equation}

\begin{corolario}
	\textit{El problema} \textbf{PR} \textit{es el problema relajado de} \textbf{PP} \textit{, para $\lambda_{i}\geq 0$ para $i=1,\dots ,n$.}	
\end{corolario}

{\raggedright\textbf{DEMOSTRACIÓN:}}

Para que sea problema relajado la región factible del problema \textbf{(PR)} debe ser al menos tan grande como la del problema \textbf{(PP)}, esto se verifica, ya que el conjunto de valores tal que $g_{i}(\textbf{x})\leq0$ para $\textbf{x}\in\mathbb{R}^{m}$ está contenido en el conjunto $\mathbb{R}^{m}$.\\

Por otro lado, debe cumplir que la función objetivo del problema \textbf{(PR)} es menor o igual que la del problema \textbf{(PP)}, esto se puede comprobar aplicando que como la función $g_{i}(\textbf{x})\leq 0$ entonces $\sum_{i=1}^{n}\lambda_{i}g_{i}(\textbf{x})\leq 0$.

\begin{equation}\nonumber
 f(\textbf{x})+\sum_{i=1}^{n}\lambda_{i}g_{i}(\textbf{x}) \leq f(\textbf{x})
\end{equation}
$\hfill\square$\\

Como se puede observar, la variable definida en el corolario anterior como $\lambda_{i}$ corresponde con los multiplicadores de Lagrange $\alpha_{i}$, dados en la Definición \ref{definicionfuncionlagrange}.\\


Ahora se tiene un problema relajado que corresponde con una cota inferior del problema \textbf{(PP)}, pero se busca la mayor de dichas cotas en función de todos los posibles valores de $\lambda_{i}$, para ello resolvemos el problema dual dado por
\begin{alignat*}{3}
\textbf{(PD)} \quad & \max_{\boldsymbol{\lambda}} \quad && \inf_{\textbf{x}\in\mathbb{R}^{m}} L(\textbf{x},\boldsymbol{\lambda})=f(x)+\sum_{i=1}^{n}\lambda_{i}g_{i}(x) &&\\
& \quad &&  \alpha_{i}\geq 0, \quad  i=1,\dots,n. \quad && \\
\end{alignat*}

En el caso que nos atañe, el problema relajado tiene como función objetivo la expresión \ref{lagrange}, siendo su problema dual 
\begin{alignat*}{3}
\textbf{(PD1)} \quad & \max_{\boldsymbol{\alpha}}   \quad && \inf_{\textbf{v}} L(\textbf{v},a,\boldsymbol{\alpha})= \inf_{\textbf{v}} \frac{1}{2}\norm{\textbf{v}}^{2}-\sum_{i=1}^{n}\alpha_{i}(y_{i}(<\textbf{v},\textbf{x}_{i}>+a)-1)  &&\\
& \quad &&  \alpha_{i}\geq 0, \quad  i=1,\dots,n. \quad && \\
\end{alignat*}

Antes de pasar a demostrar el teorema de Karush-Kuhn-Tacker se verán conceptos básicos necesarios para la demostración del teoirema. Por otro lado, se mostrará la relación entre la solución del problema dual \textbf{(PD1)} y el problema primal \textbf{(P1)}; Para esto último, se necesita demostrar el siguiente teorema.\\

\begin{teorema}[label={teorema311},nameref={Title or anything else}]
	\textit{}
	\textit{Sean $\boldsymbol{\alpha}$ y \textbf{x} vectores que satisfacen las restricciones del problema dual} \textbf{(PD)}\textit{ y del primal} \textbf{(PP)} \textit{respectivamente, es decir, $\boldsymbol{g_{i}(\textbf{x})\leq 0}$ y $\boldsymbol{\alpha_{i}\geq 0}$, con $\boldsymbol{i}$=1,\dots,n, entonces $\boldsymbol{\varphi(\alpha)} \leq f(\textbf{x})$, siendo $$\boldsymbol{\varphi(\alpha)=\inf_{\textbf{x}}} \hspace{2mm}\textbf{L(x,$\boldsymbol{\alpha}$)}$$}
		
\end{teorema}

{\raggedright\textbf{DEMOSTRACIÓN:}}

Para la demostración bastará con desarrollar las expresiones.

$$\varphi(\boldsymbol{\alpha})= \inf_{\textbf{x}\in\mathbb{R}^{m}} L(\textbf{x},\boldsymbol{\alpha}) = \inf_{\textbf{x}\in\mathbb{R}^{m}} f(\textbf{x}) + \sum_{i=1}^{n}\alpha_{i}g_{i}(\textbf{x}).$$

Por hipótesis, sabemos que $\alpha_{i}\geq0$ y $g_{i}(\textbf{x})\leq0$ para $i=1,\dots,n$, y por tanto $\alpha_{i}g_{i}(\textbf{x})\leq0$, luego $\sum_{i=1}^{n}\alpha_{i}g_{i}(\textbf{x})\leq0$, aplicando esta desigualdad obtenemos \\
$$\varphi(\boldsymbol{\alpha})= \inf_{\textbf{x}\in\mathbb{R}^{m}} f(\textbf{x}) + \sum_{i=1}^{n}\alpha_{i}g_{i}(\textbf{x})\leq \inf_{\textbf{x}\in\mathbb{R}^{m}} f(\textbf{x}). $$

Por último, es trivial ver que $$\inf_{\textbf{x}\in\mathbb{R}^{m}} f(\textbf{x})\leq f(\textbf{x}).$$
$\hfill\square$

% revisar si las variables en el teorema están bien puestas.

Del teorema anterior se pueden deducir 2 corolarios.\\

\begin{corolario}
	\textit{El problema dual} \textbf{(PD)} \textit{está acotado superiormente por el problema primal} \textbf{(PP)}.	
\end{corolario}
{\raggedright\textbf{DEMOSTRACIÓN:}}

Es trivial, puesto que la función $\varphi(\boldsymbol{\alpha})$ corresponde con la función objetivo del problema dual y $f(\textbf{x})$ la del problema primal, aplicando el teorema anterior \ref{teorema311} tenemos que 
$$\varphi(\boldsymbol{\alpha})\leq f(\textbf{x}).$$
$\hfill\square$\\



\begin{corolario}[label={corolario313},nameref={Title or anything else}]
	\textit{Sea} \textbf{(PP)}\textit{ el problema primal definido anteriormente y} \textbf{(PD)} \textit{su correspondiente problema dual. Si $\varphi(\boldsymbol{\alpha})=f(\textbf{x})$, entonces $\boldsymbol{\alpha}$ y $\textbf{x}$ son soluciones, respectivamente del problema} \textbf{(PD)} \textit{y} \textbf{(PP)}.
\end{corolario}


{\raggedright\textbf{DEMOSTRACIÓN:}}

Sabemos que $\varphi(\boldsymbol{\alpha})\leq f(\textbf{x})$ por el teorema \ref{teorema311}. La función objetivo del problema primal, $f(\textbf{x})$, está minimizando y la del problema dual, $\varphi(\boldsymbol{\alpha})$, está maximizando. Supongamos que 
$$\varphi(\boldsymbol{\alpha}) = f(\textbf{x}),$$
esto significa que de todos los valores posibles de  $\varphi(\boldsymbol{\alpha})$, le corresponde el menor de ellos, por tanto se trata del máximo del conjunto. Mientras que $f(\textbf{x})$ tiene su máximo valor posible, siendo entonces el mínimo del problema \textbf{(P1)}. Al verificarse las funciones objetivos de sus respectivos problemas, $\boldsymbol{\alpha}$ y $\textbf{x}$ son soluciones de sus respectivos problemas.
$\hfill\square$\\

%De este último corolario podemos concluir que cuanto menor sea la diferencia entre $\varphi(\boldsymbol{\alpha})$ y $f(\textbf{x})$, más se aproximarán dichos valores $\boldsymbol{\alpha}$\ y $\textbf{x}$ a la solución de sus respectivos problemas.\\

De este último corolario, se puede ver que si se cumplen las hipótesis anteriores, es posible saber la solución de un problema y a partir de ésta obtener la solución de su problema dual. En el caso que se trata en esta sección (\textbf{¿SUBSECCIÓN?}) vamos a resolver el problema dual \textbf{(PD1)}, y apartir de este, obtendremos la solución de \textbf{(P1)}.\\

Pasemos a ver algunos conceptos que se necesitan conocer para la demostración del Teorema de Karush-Kuhn-Tucker.\\

\begin{definicion}{Función Convexa}
	\textit{Diremos que la función $f$: $\Omega \subseteq \mathbb{R}^{m} \rightarrow \mathbb{R}$ con $\Omega$ un conjunto 
	convexo no vacío, es convexa en dicho conjunto si y solo si $\forall \textbf{x},\textbf{y} \in \Omega, \lambda \in [0,1] \Longrightarrow f(\lambda \textbf{x}+ (1-\lambda)\textbf{y}) \leq \lambda f(\textbf{x})+(1-\lambda)f(\textbf{y})$.} \\
	
	\textit{Decimos que la función $f$ es estrictamente convexa cuando $f(\lambda \textbf{x}+ (1-\lambda)\textbf{y}) < \lambda f(\textbf{x})+(1-\lambda)f(\textbf{y})$.}	
\end{definicion}

En este caso, es fácil comprobar que las restricciones y la función objetivo del problema \textbf{(PD1)} son convexas, y al tratarse de funciones polinómicas son diferenciables.\\

A continuación se definen lo que es un punto crítico y un vector gradiente.\\

\begin{definicion}{Punto Crítico}
	\textit{Sea $f:\Omega\subset \mathbb{R}^{m}\rightarrow \mathbb{R}$ una función y sea $\textbf{x}=(x_{1},\dots,x_{m})\in\Omega$ un punto. Se dice que $\textbf{x}$ es un punto crítico, si sus derivadas parciales son cero, es decir} 
	
	\textit{$$\frac{\partial f (\textbf{x})}{\partial x_{i}}=0, \hspace{2mm} \text{para $i=1,\dots,n.$}$$}
\end{definicion}

\begin{definicion}{Vector Gradiente}
	\textit{Sea $f:\Omega\subset \mathbb{R}^{m}\rightarrow \mathbb{R}$ una función y sea $\textbf{x}=(x_{1},\dots,x_{m})\in\Omega$ un punto. Se define el vector gradiente de la función $f$ en el punto $(x_{1},\dots,x_{m})$ como }
	
	\textit{$$\nabla f (\textbf{x})= \left(\frac{\partial f(\textbf{x})}{\partial x_{1}},\dots,\frac{\partial f(\textbf{x})}{\partial x_{m}} \right)$$}
\end{definicion}

Ahora con los conceptos anteriores explicados, podemos pasar a demostrar el Teorema de Karush-Kuhn-Tucker.\\  

\begin{teorema}[label={teorema312},nameref={Title or anything else}]{KARUSH-KUHN-TUCKER}
	\textit{Sea $\textbf{x}^{*}$ un punto factible para el problema} \textbf{(PP)} \textit{, las funciones $f:\mathbb{R}^{m}\rightarrow \mathbb{R}$, \  $g_{i}:\mathbb{R}^{m}\rightarrow \mathbb{R}$ con $i=1,\dots,n$ \space funciones convexas y diferenciables y existen constantes $\alpha_{i}\geq0$, tales que} 
	
	\begin{equation}\label{condicion1}
	\frac{\partial f(\textbf{x}^{*})}{\partial x_{j}} + \sum_{i=1}^{n}\alpha_{i}\frac{\partial g_{i}(\textbf{x}^{*})}{\partial x_{j}}=0, \text{\textit{ para $j=1,\dots,m$}} ,
	\end{equation}
	
	\begin{equation}\label{condicion2}
	\alpha_{i}g_{i}(\textbf{x}^{*})=0, \text{ \textit{para i=1,\dots,n}} , 
	\end{equation}
	\textit{entonces el punto $\textbf{x}^{*}$ es un mínimo global del problema primal.}	
\end{teorema}


{\raggedright\textbf{DEMOSTRACIÓN:}}

Sea 
$$L(\textbf{x},\boldsymbol{\alpha})= f(\textbf{x})+\sum_{i=1}^{n}\alpha_{i}g_{i}(\textbf{x}),$$
la función de Lagrange correspondiente al problema \textbf{(PP)}, al ser $f$ y $g_{i}$ funciones convexas por hipótesis, $L(\textbf{x},\boldsymbol{\alpha})$ también lo es, por ser suma de funciones convexas.\\

Se puede observar que la expresión \eqref{condicion1} es equivalente a que el vector gradiente de $L(\textbf{x}^{*},\boldsymbol{\alpha})$ sea 0,  es decir $\nabla L(\textbf{x}^{*},\boldsymbol{\alpha})=0$, y por tanto $\textbf{x}^{*}$ es un punto crítico; Al ser $L$ una función convexa, el punto crítico se trata de un mínimo. y por tanto $L(\textbf{x}^{*},\boldsymbol{\alpha})\leq L(\textbf{x},\boldsymbol{\alpha})$ para $\forall \textbf{x}\in \mathbb{R}^{m}$.\\

% \textbf{NO SE SI PONER EN VEZ DE ESO, $\Omega$}\\

Al ser $\alpha_{i}g_{i}(\textbf{x}^{*})=0$ para $i=1,\dots,n$, entonces la suma de todas los elementos es 0. Por otro lado del problema \textbf{(PP)} sabemos que $g_{i}(x)\leq 0$ para $i=1,\dots,n$ y $\textbf{x}\in \Omega$ y $\alpha_{i}\geq0$, por lo tanto $\alpha_{i}g_{i}(x)\leq0$.\\

Aplicando \eqref{condicion1} e \eqref{condicion2} tenemos que
\begin{equation}\nonumber
f(\textbf{x}^{*})=f(\textbf{x}^{*})+\sum_{i=1}^{n}\alpha_{i}g_{i}(\textbf{x}^{*})=L(\textbf{x}^{*},\alpha)\leq L(\textbf{x},\alpha)=f(\textbf{x})+\sum_{i=1}^{n}\alpha_{i}g_{i}(\textbf{x})\leq f(\textbf{x}).
\end{equation}
 $\hfill\square$\\
%no se como carajos demostrarlo

La primera de las condiciones del Teorema de Karush-Kuhn-Tucker \ref{teorema312} corresponde con el ínfimo de la función de Lagrange, en el caso que se está estudiando en función de las variables $\textbf{\textbf{v}}$ y $a$, por otro lado, la segunda condición es necesaria, ya que se busca que la función objetivo del problema primal y la del dual sean iguales para aplicar el corolario \ref{corolario313}.\\

Aplicamos la primera de las condiciones KKT, como tenemos solamente la función de Lagrange hacemos las derivadas parciales con respecto $\textbf{v}$ y $a$.\\
\begin{equation}\label{derivada1}
\frac{\partial L(\textbf{v}^{*},a^{*},\boldsymbol{\alpha}^{*})}{\partial \textbf{v}}= \textbf{v}^{*}-\sum_{i=1}^{n}\alpha_{i}^{*}\textbf{x}_{i}y_{i}=0, \qquad i=1,\dots,n,
\end{equation}

\begin{equation}\label{derivada2}
\frac{\partial L(\textbf{v}^{*},a^{*},\boldsymbol{\alpha}^{*})}{\partial a}=- \sum_{i=1}^{n}\alpha_{i}^{*}y_{i}=0, \qquad i=1,\dots,n.
\end{equation}


De la ecuación \eqref{derivada1} podemos deducir que 
\begin{equation}\label{conclusionderivada1}
\textbf{v}^{*}= \sum_{i=1}^{n}\alpha_{i}^{*}\textbf{x}_{i}y_{i}.
\end{equation}
A continuación aplicamos la segunda de las condiciones KKT obteniendo lo siguiente
\begin{equation}\label{segundacondicion2}
\alpha_{i}^{*}(1-y_{i}(<\textbf{v}^{*},\textbf{x}_{i}>+a^{*}))=0.
\end{equation}

%es -g, porque parte de un problema en la que g<=0 y en nuestro caso es g>=0

Sustituimos las ecuaciones \eqref{derivada2} , \eqref{conclusionderivada1} en \eqref{lagrange}.\\
\begin{equation}\label{desarrollolagrange}\nonumber
\begin{split}
L(\boldsymbol{\alpha})& =\frac{1}{2}<\textbf{v}^{*},\textbf{v}^{*}>-\sum_{i=1}^{n}\alpha_{i}^{*}(y_{i}(<\textbf{v}^{*},\textbf{x}_{i}>+a^{*})-1)=\\
&=\frac{1}{2}<\textbf{v}^{*},\textbf{v}^{*}>-\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}<\textbf{v}^{*},\textbf{x}_{i}> -\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}a^{*} + \sum_{i=1}^{n}\alpha_{i}^{*}=\\
&=\frac{1}{2}\left(\sum_{i=1}^{n}\alpha_{i}^{*}\textbf{x}_{i}y_{i}\right)\left(\sum_{j=1}^{n}\alpha_{j}^{*}\textbf{x}_{j}y_{j}\right)- \left(\sum_{i=1}^{n}\alpha_{i}^{*}\textbf{x}_{i}y_{i}\right)\left(\sum_{j=1}^{n}\alpha_{j}^{*}\textbf{x}_{j}y_{j}\right) +\sum_{i=1}^{n}\alpha_{i}^{*}=\\
&=-\frac{1}{2}\left(\sum_{i=1}^{n}\alpha_{i}^{*}\textbf{x}_{i}y_{i}\right)\left(\sum_{j=1}^{n}\alpha_{j}^{*}\textbf{x}_{j}y_{j}\right)=\\
&=\sum_{i=1}^{n}\alpha_{i}^{*}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}^{*}\alpha_{j}^{*}y_{i}y_{j}<\textbf{x}_{i}\textbf{x}_{j}>.
\end{split}
\end{equation}

%no se si la puedo de notar así

Como resultado de la primera condición podemos construir el problema dual que se muestra a continuación, en función únicamente de los multiplicadores de Lagrange
%\begin{equation}\label{problemafinalseparable}
%\begin{split}
%& \text{máx}   \quad \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}<\textbf{x}_{i}\textbf{x}_{j}> \qquad x\in \Omega,y \in Y\\
%& \text{s.a}   \quad  \sum_{i=1}^{n}\alpha_{i}y_{i}=0 \quad  i=1,\dots,n\\
%&\qquad  \alpha_{i}\geq 0, i=1,\dots,n
%\end{split}
%\end{equation}
\begin{alignat*}{3}
\textbf{(PD2)} \quad & \max_{\boldsymbol{\alpha}}  \quad && \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}<\textbf{x}_{i}\textbf{x}_{j}> \qquad&& \\
& \text{s.a}   \quad && \eqref{derivada2}, \quad && \\
&\qquad && \alpha_{i}\geq 0,\quad i=1,\dots,n, \quad \textbf{x}_{i}\in \Omega,\quad y_{i} \in Y.
\end{alignat*}

Al resolver el problema \textbf{(PD2)} se obtiene el valor de $\boldsymbol{\alpha}$, que sustituyendo en la expresión \eqref{conclusionderivada1} nos daría el valor de \textbf{v}, por tanto solo faltaría conocer el valor de $a$ para conseguir la expresión del hiperplano definido como \eqref{hiperplanooptimo}.\\

Volviendo a la expresión deducida de la segunda condición del Teorema KKT \eqref{segundacondicion2}, podemos concluir que cuando $\alpha_{i}>0$, entonces $1-y_{i}(<\textbf{v}^{*},\textbf{x}_{i}>+a^{*})=0$, por lo que 
\begin{equation}\label{deduccioncondicion2}
\begin{split}
y_{i}(<\textbf{v}^{*},\textbf{x}_{i}>+a^{*})=1,
\end{split}
\end{equation}
por tanto la dupla $(\textbf{x}_{i},y_{i})$ para $i=1,\dots,n$ verifica el problema \textbf{(P1)}, ya que al aplicar las condiciones del Teorema de Karush-Kuhn-Tucker  \ref{teorema312} $, \textbf{v}^{*}$ es un mínimo del problema \textbf{(P1)} y la restricción de dicho problema es \eqref{hiperplanooptimo}, por lo que\eqref{deduccioncondicion2} verifica la igualdad.\\

La traslación de nuestro hiperplano, $a$ , la podemos despejar de la ecuación \eqref{deduccioncondicion2}
\begin{equation}\label{a}\nonumber
\begin{split}
a^{*}=y_{i}-<\textbf{v}^{*},\textbf{x}_{i}>.
\end{split}
\end{equation}

Luego la expresión del hiperplano óptimo es la siguiente

\begin{equation}\label{hiperplanofinalseparable}
H(\textbf{x})=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}<\textbf{x}_{i},\textbf{x}> +y_{i}-<\textbf{v}^{*},\textbf{x}_{i}>,   \qquad i=1,\dots ,n.
\end{equation}

Se concluye por la Definición \ref{vectoresoporte} que las duplas $(\textbf{x}_{i},y_{i})$ para $i=1,\dots,n$ que les corresponda $\alpha_{i}>0$ son vectores soporte, por tanto el hiperplano de separación está definido a partir de vectores soporte.\\


%poner que es mejor con la suma de los vectores soporte.

\subsection{CUASI-SEPARABLE}

Como se explicó a principio de sección, el tipo de formulación del SVM depende de la relación que tengan los elementos de $\Omega$, y por tanto de si se pueden separar las clases con hiperplanos lineales o no. Explicaremos ahora el caso en el que las clases se puedan separar mediante un hiperplano lineal, pero permitiendo que haya algunos errores de clasificación.\\

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{datoscuasiseparablehiperplano}
	\caption{Ejemplo de un hiperplano que separa ambas clases, pero no están perfectamente separadas.}	
\end{figure}
Esto lo representaremos añadiendo una variable de holgura a la ecuación \eqref{hiperplanooptimo},\newline $\xi_{i} \geq 0$ para $i=1,\dots,n$,  \space dicha variable penalizará los datos que no han sido correctamente clasificados.
\begin{equation}\label{hiperplanocuasi}\nonumber
\begin{split}
 y_{i}(<\textbf{v},\textbf{x}_{i}>+a)\geq 1-\xi_{i}.
\end{split}
\end{equation}

%no tengo muy claro porque el signo negativo

Se puede deducir que cuantos más errores cometa el problema, mayor será $\sum_{i=1}^{n} \xi_{i}$, por tanto la función objetivo no puede ser igual a la del caso anterior, ya que ahora no solo hay que maximizar el margen, sino que también hay que minimizar la variable $\boldsymbol{\xi}$ que penaliza los errores. La función objetivo pasaría a ser la siguiente
\begin{equation}\label{funcionobjetivomincuasi}
\begin{split}
\frac{1}{2}\norm{\textbf{v}}^{2} + C\sum_{i=1}^{n}\xi_{i},
\end{split}
\end{equation}
con $C$ una constante para controlar los parámetros $\boldsymbol{\xi}$ y $\textbf{v}$. Nuestro problema pasaría a ser 
%\begin{equation}\label{cuasiproblema1}
%\begin{split}
%& \text{min}   \quad  \frac{1}{2}\norm{\textbf{v}}^{2}+C\sum_{i=1}^{n}\xi_{i} \\
%& \text{s.a}   \quad  y_{i}(<\textbf{v},\textbf{x}_{i}> + a)-1 + \xi_{i} \geq 0\\
%& \quad \quad \xi_{i}\geq 0 \quad i=1,\dots,n 
%\end{split}
%\end{equation}
\begin{alignat*}{3}
\textbf{(PPC)} \quad & \text{min}   \quad && \frac{1}{2}\norm{\textbf{v}}^{2}+C\sum_{i=1}^{n}\xi_{i} \qquad&& \\
& \text{s.a}   \quad && \eqref{hiperplanocuasi}, \quad && \\
&\qquad && \xi_{i}\geq 0, \quad i=1,\dots,n 
\end{alignat*}

Al igual que cuando existe un hiperplano lineal que separa las clases sin permitir errores, aplicamos el método de los multiplicadores de Lagrange, siendo la función de Lagrange en este caso
\begin{equation}\label{Lagrangecuasi}
L(\textbf{v},a,\boldsymbol{\xi},\boldsymbol{\alpha},\boldsymbol{\beta})=\frac{1}{2}\norm{\textbf{v}}^{2}+ C\sum_{i=1}^{n}\xi_{i}-\sum_{i=1}^{n}\alpha_{i}[y_{i}(<\textbf{v},\textbf{x}_{i}>+a)-1+\xi_{i}]-\sum_{i=1}^{n}\beta_{i}\xi_{i},
\end{equation}
ahora se tienen dos tipos diferentes de multiplicadores de lagrange, $\alpha_{i},\beta_{i}\geq 0$ para \newline$i=1,\dots,n$, debido a que el problema \textbf{(PPC)} tiene 2 grupos de restricciones.\\

De nuevo como en el caso anterior, aplicamos el Teorema de Karush-Kuhn-Tucker \ref{teorema312}, primero se calculará la primera de las condiciones, \eqref{condicion1}, para ello hacemos la derivada de \eqref{Lagrangecuasi} con respecto a las variables $\textbf{v},a$ y $\xi_{i}$
%entiendo que lo hago respecto a las componentes,porque no está expresado en ningún momento como el conjunto no?
\begin{equation}\label{condicion1cuasi1}
\begin{split}
& \frac{\partial L(\textbf{v}^{*},a^{*},\boldsymbol{\xi}^{*},\boldsymbol{\alpha}^{*},\boldsymbol{\beta})}{\partial \textbf{v}}=\textbf{v}^{*}-\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}\textbf{x}_{i}=0,  \qquad i=1,\dots,n,  \\
\end{split}
\end{equation}

\begin{equation}\label{condicion1cuasi2}
\begin{split}
& \frac{\partial L(\textbf{v}^{*},a^{*},\boldsymbol{\xi}^{*},\boldsymbol{\alpha}^{*},\boldsymbol{\beta})}{\partial a} =-\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}=0,  \qquad i=1,\dots,n,  \\
 \end{split}
\end{equation}

\begin{equation}\label{condicion1cuasi3}
\begin{split}
& \frac{\partial L(\textbf{v}^{*},a^{*},\boldsymbol{\xi}^{*},\boldsymbol{\alpha},\boldsymbol{\beta})}{\partial \boldsymbol{\xi}_{i}}=C-\alpha_{i}^{*}-\beta_{i}=0,  \qquad i=1,\dots,n.
\end{split}
\end{equation}

De las expresiones anteriores se pueden sacar algunas conclusiones. De \eqref{condicion1cuasi1} concluimos que 
\begin{equation}\label{deduccioncondicion1cuasi1}
\textbf{v}^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}\textbf{x}_{i}, \quad i=1,\dots,n.
\end{equation}

Por otro lado de \eqref{condicion1cuasi2} y \eqref{condicion1cuasi3} se deduce respectivamente que 
\begin{equation}\label{deduccioncondicion1cuasi2}
\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}=0,  \qquad i=1,\dots,n,
\end{equation}
\begin{equation}\label{deduccioncondicion1cuasi3}
C=\alpha_{i}^{*}+\beta_{i},  \qquad i=1,\dots,n.
\end{equation}
 

A continuación desarrollamos lo que correspondería a la segunda condición del teorema KKT, \eqref{condicion2}. En el caso que se está tratando se tienen dos ecuaciones, una por cada restricción de nuestro problema \textbf{(PPC)}

\begin{equation}\label{condicion2cuasi1}
\alpha_{i}^{*}[y_{i}(<\textbf{v}^{*},\textbf{x}_{i}>+a^{*})-1+\xi_{i}^{*}]=0, \qquad i=1,\dots,n ,
\end{equation}

\begin{equation}\label{condicion2cuasi2}
\beta_{i}\xi_{i}^{*}=0, \qquad i=1,\dots,n. 
\end{equation}

Sustituimos \eqref{deduccioncondicion1cuasi1}\space ,\space  \eqref{deduccioncondicion1cuasi2}\space , \space \eqref{deduccioncondicion1cuasi3} en la expresión \ref{Lagrangecuasi}
\begin{equation}\label{desarrollolagrange}\nonumber
\begin{split}
L(\textbf{v},a,\boldsymbol{\xi},\boldsymbol{\alpha},\boldsymbol{\beta})&
=\frac{1}{2}\norm{\textbf{v}}^{2}+ C\sum_{i=1}^{n}\xi_{i}-\sum_{i=1}^{n}\alpha_{i}[y_{i}(<\textbf{v},\textbf{x}_{i}>+a)-1+\xi_{i}]-\sum_{i=1}^{n}\beta_{i}\xi_{i}=\\
&=\frac{1}{2}<\textbf{v},\textbf{v}>-\sum_{i=1}^{n}\alpha_{i}y_{i}<\textbf{v},\textbf{x}_{i}>+\sum_{i=1}^{n}\alpha_{i}+\sum_{i=1}^{n}\xi_{i}(C-\alpha_{i}-\beta_{i})=\\
&=\frac{1}{2}\left(\sum_{i=1}^{n}\alpha_{i}y_{i}\textbf{x}_{i}\right)\left(\sum_{j=1}^{n}\alpha_{j}y_{j}\textbf{x}_{j}\right)-\left(\sum_{i=1}^{n}\alpha_{i}y_{i}\textbf{x}_{i}\right)\left(\sum_{j=1}^{n}\alpha_{j}y_{j}\textbf{x}_{j}\right)+\sum_{i=1}^{n}\alpha_{i}=\\
&=-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}<\textbf{x}_{i},\textbf{x}_{j}>+\sum_{i=1}^{n}\alpha_{i}.
\end{split}
\end{equation}

Se obtiene una formulación con $\alpha_{i}\geq0$ para $i=1,\dots,n$, las únicas variables desconocidas. A partir de \eqref{deduccioncondicion1cuasi3} se deduce que $\alpha_{i}\leq C$, debido a $\beta_{i}\geq 0$ para $i=1,\dots,n$. Por tanto el problema a resolver es el siguiente
%\begin{equation}\label{problemafinalcuasi}
%\begin{split}
%& \text{máx}   \quad -\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}<\textbf{x}_{i},\textbf{x}_{j}>+\sum_{i=1}^{n}\alpha_{i} \qquad \textbf{x}_{i}\in \Omega,y \in Y\\
%& \text{s.a}   \quad  \sum_{i=1}^{n}\alpha_{i}y_{i}=0 \quad  i=1,\dots,n\\
%&\qquad  0\leq \alpha_{i}\leq C, i=1,\dots,n
%\end{split}
%\end{equation}
\begin{alignat*}{3}
\textbf{(PDC)} \quad & \max_{\boldsymbol{\alpha}}   \quad && -\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}<\textbf{x}_{i},\textbf{x}_{j}>+\sum_{i=1}^{n}\alpha_{i} \quad&&  \\
& \text{s.a}   \quad && \eqref{condicion1cuasi2} \quad &&\\
&\qquad && 0\leq \alpha_{i}\leq C,\quad  i=1,\dots,n, \quad \textbf{x}_{i}\in \Omega,\quad y_{i} \in Y.
\end{alignat*}

Análogo al caso de clases linealmente separables, para hallar el hiperplano separador, hay que sustituir la expresión \eqref{deduccioncondicion1cuasi1} en \eqref{hiperplano} obteniendo

\begin{equation}\label{hiperplanonecesariosiguientecaso}
H(x)=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}<\textbf{x}_{i},\textbf{x}> + a 
\end{equation}

El valor de $a$ depende de $\alpha_{i}$ para $i=1,\dots,n$ como se puede observar en la expresión anterior, debido a que $\alpha_{i}\in[0,C]$ para $i=1,\dots,n$, estudiaremos para que valores de $\alpha_{i}$ se definirá $a$.\\

 Existen tres posibles casos que, $\alpha_{i}=0$, \hspace{1mm} $\alpha_{i}=C$ \hspace{1mm} o \hspace{1mm} $0<\alpha_{i}<C$.  \hspace{1mm}Si $\alpha_{i}=0$ entonces por la expresión \eqref{deduccioncondicion1cuasi3} se obtiene que $\beta_{i}=C$ y por tanto que $\xi_{i}^{*}=0$ por \eqref{condicion2cuasi2}, luego en este caso el hiperplano sería análogo al caso linealmente separables \eqref{hiperplanofinalseparable}.\\
 
 Si $\alpha_{i}=C$, entonces debido a \eqref{condicion2cuasi1} se deduce que 
 
 \begin{equation*}
 y_{i}(<\textbf{v}^{*},\textbf{x}_{i}>+a^{*})-1+\xi_{i}^{*}=0,
 \end{equation*}
 
	por lo que despejando $\xi_{i}^{*}$ obtenemos 
	
 \begin{equation}\label{valorholgura final}
	\xi_{i}^{*}=1-y_{i}(<\textbf{v}^{*},\textbf{x}_{i}>+a^{*}).
 \end{equation}

Aquí se pueden considerar dos situaciones distintas que $x_{i}$ esté bien clasificado, o que esté mal clasificado.

\newpage
\begin{figure}[h]	
	\centering
	\subfloat[]{
		
		\includegraphics[width=0.45\textwidth]{datosbienclasificadoscuasi}
		\label{figura341}	
	}
	\subfloat[]{
		
		\includegraphics[width=0.48\textwidth]{datosmalclasificadoscuasi}	
		\label{figura342}
	}
	\caption{Las imágenes corresponden a los casos en los que $x_{i}$ se encuentre bien clasificado \ref{figura341} o que esté mal clasificado \ref{figura342}}
\end{figure}

En la primera de las situaciones expuestas anteriormente se tendría que al estar bien clasificado $y_{i}H(\textbf{v},\textbf{x}_{i})\geq0$, por lo que el valor de $\xi_{i}^{*}$ pasaría a ser 

$$\xi_{i}^{*}=1-\abs{<\textbf{v}^{*},\textbf{x}_{i}>+a^{*}}.$$

En el caso de que esté mal clasificado se tendría que $y_{i}H(\textbf{v},\textbf{x}_{i})\leq 0$, por lo que se tendría

$$\xi_{i}^{*}=1+\abs{<\textbf{v}^{*},\textbf{x}_{i}>+a^{*}}.$$

Por último estudiemos el caso en el que $0<\alpha_{i}<C$, se puede deducir de la expresión \eqref{deduccioncondicion1cuasi3} que $\beta_{i}\neq0$, por lo que se concluye que $\xi_{i}=0$ de \eqref{condicion2cuasi2}. De la expresión \eqref{condicion2cuasi1} se puede despejar que 
\begin{equation*}
y_{i}(<\textbf{v}^{*},\textbf{x}_{i}>+a^{*})-1=0,
\end{equation*}
 y despejando $a^{*}$ obtenemos 
 \begin{equation*}
 a^{*}=y_{i}-<\textbf{v}^{*},\textbf{x}_{i}>.
 \end{equation*}
 
 Por lo tanto conociendo la expresión de $a^{*}$ y $\textbf{v}^{*}$ se puede expresar el hiperplano óptimo como 
 
\begin{equation*}
H(\textbf{x})=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}<\textbf{x}_{i},\textbf{x}> +y_{i}-<\textbf{v}^{*},\textbf{x}_{i}>,   \qquad i=1,\dots ,n.
\end{equation*}
\subsection{LINEALMENTE NO SEPARABLE}\label{LNS}

\textbf{QUIERO PONER IMÁGENES, LAS ESTOY HACIENDO, CUANDO LAS TENGA COMPLETADAS, LAS AÑADIRÉ POR EL TEXTO}.\\

Cuando las clases no puedan ser separadas por un hiperplano lineal, permitiendo algún o ningún dato mal clasificado, lo que se buscará es un hiperplano no linea que las separe. Para ello la idea es trasladarlos a una dimensión superior, en la que el hiperplano sí sea lineal y se puedan separar las clases fácilmente.\\

Para ello definiremos la función no lineal $\Phi$ que será la que transportará los elementos $\textbf{x}\in\Omega\subset\mathbb{R}^{n}$ a otra dimensión superior

\begin{alignat*}{3}
\boldsymbol{\Phi}: \Omega\subset\mathbb{R}^{n}  & \rightarrow \quad && L \\
\quad \textbf{x} 			& \rightarrow \quad &&  (\Phi_{1}(\textbf{x}),\dots, \Phi_{k}(\textbf{x})).
\end{alignat*}

Donde  $\Phi_{i}$ con $i=1,\dots,k$, siendo $k\geq n+1$, son funciones no lineales y $\Phi_{1}(x)$ corresponderá al término independiente $a$ usado en \eqref{hiperplano}, por tanto ahora el hiperplano no lineal que se busca es 

\begin{equation*}
	H(x)=(v_{1}\Phi_{1}(\textbf{x})+\dots+ v_{m}\Phi_{m}(\textbf{x}))=<\textbf{v},\boldsymbol{\Phi}(\textbf{x})>.
\end{equation*}

A continuación se definirá lo que denotaremos a partir de ahora como función Kernel y será utilizada en lugar del producto escalar $<\cdot,\cdot>$.\\

\begin{definicion}{Función Kernel}
	\textit{Sea $\boldsymbol{\Phi}:\Omega\subset\mathbb{R}^{n} \rightarrow L$ una función que transporta los valores de $\Omega$ a una superficie de mayor dimensión $L$. Se define la función Kernel como la función    $K:\Omega\times\Omega \rightarrow \mathbb{R}$ con $(\textbf{x},\textbf{y})\in\Omega\times\Omega$ tal que verifique} 
	
	$$K(\textbf{x},\textbf{y})=<\boldsymbol{\Phi}(\textbf{x}),\boldsymbol{\Phi}(\textbf{y})>=\Phi_{1}(\textbf{x})\Phi_{1}(\textbf{y})+\dots+\Phi_{k}(\textbf{x})\Phi_{k}(\textbf{y}).$$
 \end{definicion}

Las funciones $\Phi_{i}$, con $i=1,\dots$ pueden llegar a ser infinitas, el como se puede transformar un conjunto de entrada de dimensión finita a otro de dimensión infinita se puede ver a través del siguiente Teorema.\\

\begin{teorema}[label={teorema312},nameref={Title or anything else}]{Aronszajn}
 	\textit{Para cualquier función $K:\Omega \times \Omega \rightarrow \mathbb{R}$ que sea simétrica y semidefinida positiva, existe un espacio de Hilbert y una función $\Phi:\Omega \rightarrow L$ tal que}
 	
 	$$K(\textbf{x},\textbf{y})=<\Phi(\textbf{x}),\Phi(\textbf{y})>, \hspace{2mm} \forall \textbf{x},\textbf{y}\in \Omega$$ 	
\end{teorema} 

Por tanto, no es necesario saber la expresión de la función $\Phi$ para conocer a $K$, basta comprobar que $K$ es simétrica y definida positiva para poder decir que existe un producto escalar de funciones $\Phi$.\\

Teniendo en cuenta que en el espacio transformado $L$, si existe un hiperplano lineal que separe las clases, se puede deducir que el problema a resolver sería análogo al caso en el que los datos sean linealmente separables, con la excepción de que en vez de $<\cdot,\cdot>$ se usará la función Kernel $K(\cdot,\cdot)$. Por tanto el problema a resolver es

\begin{alignat*}{3}
\textbf{(PD-NS)} \quad & \max_{\boldsymbol{\alpha}}  \quad && \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}K(\textbf{x}_{i},\textbf{x}_{j})\qquad&& \\
& \text{s.a}   \quad && \eqref{derivada2}, \quad && \\
&\qquad && \alpha_{i}\geq 0,\quad i=1,\dots,n, \quad \textbf{x}_{i}\in \Omega,\quad y_{i} \in Y.
\end{alignat*}

El hiperplano óptimo para separar las clases en el espacio $L$ vendría dado por la expresión \eqref{hiperplanooptimo}, su deducción es similar al caso anterior donde el hiperplano venia dado por la expresión \eqref{hiperplanonecesariosiguientecaso}, con la sustitución de la función Kernel, en donde se encontraba el producto escalar

\begin{equation*}\label{vnoseparable}
\textbf{v}^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}K(\textbf{x}_{i},\textbf{x})
\end{equation*}

Equivalentemente al caso linealmente separable, se deduce que la variable $a^{*}$ viene dada por 

\begin{equation*}\label{anoseparable}
a^{*}=y_{i}-K(\textbf{v}^{*},\textbf{x}_{i})
\end{equation*}

Luego el hiperplano óptimo vendría dado por la siguiente expresión

\begin{equation*}\label{anoseparable}
H(\textbf{x})= \sum_{i=1}^{n}\alpha_{i}^{*}y_{i}K(\textbf{x}_{i},\textbf{x})+y_{i}-K(\textbf{v}^{*},\textbf{x}_{i})
\end{equation*}
\section{Otra que no vimos}


\subsection{Otra cosita}




% ----------------------------------------------------------------------

