% !TeX encoding = ISO-8859-1


\chapter{SVM MULTICLASE}
\label{cha:svmmulticlase}

Desarrollado el algoritmo SVM, se ha observado que una de las condiciones necesarias para su uso es que haya únicamente dos clases en el conjunto de datos, pero en la realidad también nos encontramos situaciones en la que se buscará dividir los datos en un número mayor de clases; un ejemplo de esto sería la siguiente situación. Supongamos que tenemos una cuenta de correo electrónico y queremos que los correos entrantes se clasifique en función de quien ha escrito el correo, el asunto que tenga... y se archiven en las carpetas denominadas como Amigos, Familia, Trabajo, Hobbies y Otros, como se puede deducir, en este caso se busca clasificar los datos en cinco clases y por tanto no podríamos aplicar el algoritmo SVM. \\

%El ejemplo podría ser también sobre una aplicación que predice el tiempo que va a hacer y las categorías sean soleado, lluvioso, nublado y con nieve

En esta sección nos centraremos en desarrollar modelos basados en el SVM que clasifiquen utilizando un número mayor de clases, para ello introducimos lo que denominaremos como SVM-Multiclase, un conjunto de modelos similares al SVM con la diferencia de que la variable $y_{n}$ perteneciente a la dupla $(\textbf{x}_{n},y_{n})$ no pertenece a un conjunto binario, sino a un conjunto de $k$ elementos, $\{0,1,\dots,k\}$,  así el conjunto de observaciones se puede dividir en $k$ clases distintas. En la literatura existen varios enfoques distintos para abordar el SVM-Multiclase, estos pueden clasificarse en dos grandes categorías: 

\begin{itemize}
	\item Indirecto: Busca utilizar los clasificadores binarios que se han visto en la Sección \ref{cha:SVM} , aplicándolos para la creación de diversas clases artificiales independientes (por ejemplo pertenecer o no una determinada clase) que clasificaran los datos en varias variables.
	\item Directo: El objetivo es desarrollar nuevos modelos basados en el SVM en los que se considere simultáneamente la clasificación en las distintas clases, pudiendo así introducir las observaciones sin necesidad de alterarlas mediante la creación de clases artificiales. 
\end{itemize}

A continuación se desarrollarán ambos métodos junto con ejemplos para facilitar su comprensión, se empezará con el método Indirecto


\section{MÉTODO INDIRECTO}

Como se ha explicado anteriormente, en el método indirecto el objetivo es clasificar las observaciones en varias clases utilizando múltiples clasificadores binarios, para ello se construirán nuevas clases artificiales independientes que permitirán construir el clasificador que aplicaremos a las observaciones. Se usan principalmente tres métodos para obtener las dos clases artificiales necesarias para aplicar el clasificador binario.

\begin{itemize}
	\item One vs One (OVO)
	\item One vs All (OVA)
	\item Directed Acyclic Graph SVM (DAGSVM) 
\end{itemize} 

En las siguientes subsecciones se analizarán en que consiste cada método empezando por OVO y terminando por DAGSVM.

\subsection{One vs One (OVO)}

Para aplicar este método, se utiliza el SVM para separar las clases dos a dos, para ello se escogen dos clases de las $k$ disponibles a las que se le aplicará el modelo SVM, obteniendo así un hiperplano de separación entre estas dos clases. Este proceso se realizará con cada pareja de clases, es decir $\bigl(\begin{smallmatrix}k\\ 2\end{smallmatrix}\bigr)$,  lo que corresponde con $\frac{k(k-1)}{2}$ veces. Para localizar la clase a la que pertenece una nueva observación existen diferentes métodos, nosotros usaremos el primero de los que exponemos a continuación


%https://sci2s.ugr.es/ovo-ova\\

\begin{itemize}
	\item Regla Max-Wins \cite{Friedman:96}: Este método funciona como un sistema de votos en el cual cada clasificador binario da un voto a la clase a la que pertenecería según el hiperplano separador. Los votos son contados y la clase con mayor número de votos será la elegida. En caso de empate se suman las confianzas de cada clase para cada pareja y la que tenga el mayor resultado será la elegida. 
	\item Weighted voting strategy (WV): Este método es similar a la regla Max-Wins, pero en este caso cada clasificador binario vota a ambas clases y el peso de cada voto vendrá dado por la función de decisión 
	\begin{equation}\label{WV}
	\text{clase de $\textbf{x}$}= arg \stackbin[1,\dots,k]{}{\text{max}}{<\textbf{v}_{r},\textbf{x}>+a_{r}}
	\end{equation}
	donde $<\textbf{v}_{r},\textbf{x}>+a_{r}$ es el hiperplano que separa la clase $r$ del resto de clases. La clase correspondiente con el voto de mayor peso será a la que pertenezca la observación, en caso de empate la clase es escogida al azar.
	\end{itemize}
%	\item . con $<\textbf{v_{i}},\textbf{x}>+a_{i}$   % Each binary classifier votes for both classes. The weight for the vote is given by the confidence of the classifier predicting the class. The class with the largest sum value is the final output class\\	

%tras la creación de los hiperplanos, habrá que ver a cual de las dos clases que separa cada hiperplano pertenece nuestra nueva observación, obtendremos así una especie de votación formada por $\frac{k(k-1)}{2}$ votos, correspondiendo cada una de ellos a alguna de las clases, pudiéndose repetir las clases. La clase que haya obtenido más votos será a la que pertenezca la nueva observación.\\

Supongamos que disponemos de un conjunto de observaciones $(\textbf{x}_{i},y_{i})$ con $\textbf{x}_{i}\in \Omega$ e $y_{i}\in\{0,1,2\}$, por lo tanto en este caso no se puede aplicar el algoritmo SVM, ya que poseemos tres clases en lugar de dos. La representación de los datos es la siguiente\\
\begin{figure}[h]	
	\centering
	\includegraphics[width=0.7\textwidth]{OVO.eps}
	\label{figura31}	
	\caption{En la imagen se puede observar las tres clases en las que se dividen los datos, clase estrella, clase triángulo y clase cuadrado y un dato representada con un diamante morado que queremos comprobar la clase pertenece.}
\end{figure}
\newpage

Para poder aplicar el algoritmo SVM necesitamos tener únicamente dos clases, para esto veremos todas las posibles parejas de clases existentes, es decir $\bigl(\begin{smallmatrix}3\\ 2\end{smallmatrix}\bigr)=3$. Los casos posibles son la clase estrella-clase cuadrado, clase estrella-clase triángulo y clase cuadrado-clase triángulo. Al aplicar el algoritmo SVM en cada uno de los casos anteriores obtendríamos un hiperplano de separación para cada pareja, obteniendo el siguiente escenario

\begin{figure}[h]	
	\centering
	\includegraphics[width=0.7\textwidth]{OVOhiperplanos.eps}
	\label{figura32}	
	\caption{En la representación mostrada arriba se aprecian las distintas clases y los hiperplanos que separan cada pareja de clases existente. }
\end{figure}

%PONGO QUE ES PARA EL CASO DE LINEALMENTE SEPARABLE O LO EXPLICO TAMBIÉN PARA EL RESTO DE CASOS??????
La clase cuadrado no tiene ningún voto, la clase estrella tiene un voto, dado por el hiperplano estrella-cuadrado y por último la clase triángulo tiene dos votos, uno del hiperplano triángulo-estrella y otro del hiperplano triangulo-cuadrado, por tanto la clase que tiene más votos es la clase cuadrado, por lo que concluimos que es la clase a la que pertenece la observación que estábamos estudiando.\\

%\textbf{COMENTARIO: PARA LAS ZONAS EN LAS QUE NO ENCAJE EN NINGUNA DE LAS CLASES PROPUSIERON USAR REGLAS DE PARTIDO DE TENNIS DOS ARTICULOS DISTINTOS (LO HE LEIDO EN Analisys of multiclass support vector machines Abe, Shigeo, SUPONGO QUE FUNCIONA IGUAL PARA OVA )}

El ejemplo anterior corresponde con el caso en el que las observaciones $(\textbf{x}_{i},y_{i})$ son linealmente separables, pero como hemos discutido en la sección anterior no es el único caso posible. Si los datos fueran cuasi-separables o linealmente no separables el desarrollo del ejemplo sería equivalente, pero aplicando el algoritmo SVM correspondiente. 
\subsection{One vs All (OVA)}

A diferencia del método OVO, en este caso se escoge una clase $r$ en concreto y el objetivo es separarlo de las $(k-1)$ clases restantes, es decir, se crean dos clases artificiales, pertenecer a la clase $r$ o no pertenecer a la clase $r$, así se cumplen los requisitos necesarios para aplicar el SVM. Si en el conjunto de observaciones hay $k$ clases diferentes, este proceso se repetirá $k$ veces, una por cada clase existente $\bigl(\begin{smallmatrix}k\\ 1\end{smallmatrix}\bigr)$.\\

 Para hallar la clase a la que pertenece una observación existen diferentes métodos, pero en este trabajo se realizará con una estrategia de votos, su funcionamiento es el siguiente; cada clasificador binario vota a ambas clases, el peso de cada voto vendrá dado por la función de decisión \ref{WV} . La clase correspondiente con el voto de mayor peso será a la que pertenezca la observación, en caso de empate la clase escogida es al azar.\\

Como en el caso anterior se desarrollará un ejemplo para comprender los pasos a seguir para resolver el problema. Supongamos que tenemos un conjunto de observaciones linealmente separables $(\textbf{x}_{i},y_{i})$ con $\textbf{x}_{i}\in \Omega$ e $y_{i}\in\{0,1,2\}$, al no poder aplicar el algoritmo SVM, se procederá a construir dos clases artificiales, una de las cuales estará formada por dos de las tres clases previamente definidas, en total se construirán seis clases aritficiales, dos por cada clase existente, aplicando así el algoritmo SVM tres veces.\\

\begin{figure}[h]	
	\centering
	\subfloat[]{
		
		\includegraphics[width=0.45\textwidth]{OVA.eps}
		\label{figura331}	
	}
	\subfloat[]{
		
		\includegraphics[width=0.48\textwidth]{OVAhiperplanos.eps}
		\label{figura332}
	}
	\caption{La Figura \ref{figura331} muestra el conjunto de observaciones de las que disponemos junto con el ejemplo a estudiar representado por un diamante morado. Por otro lado en \ref{figura332} se puede observar las clases con los hiperplanos que las separan del resto.}
\end{figure}

En la Figura \ref{figura332} \space el color de cada hiperplano representa la clase que separa de las otras dos, teniendo por tanto tres hiperplanos resultantes del algoritmo SVM, uno por cada clase existente. Ya obtenidos los tres hiperplanos, comprobemos a que clase pertenece la observación representada por un diamante morado.
\begin{itemize}
	\item El hiperplano azul sería el siguiente $-2.00057x-1.10168=y$, al sustituir el valor del ejemplo se obtendría el valor $-3.2013$.
	\item El hiperplano naranja tendría la expresión $-0.07861x+5.74053=y$, tras sustituir el ejemplo a estudiar obtenemos el valor $-2.29556$.
	\item El hiperplano verde tiene la siguiente formulación $1.64151x+1.24594=y$ y al sustituir el ejemplo que estamos estudiando se tiene el valor $-3.40311$.
\end{itemize}

Para saber la clase a la que pertenece la observación bajo estudio aplicamos la expresión \ref{WV} y obtenemos que pertenece a la clase cuadrado.\\

El caso en el que las clases sean cuasi-separables o no separables linealmente es análogo al ejemplo que acabamos de mostrar, con la diferencia de que los hiperplanos que se buscan serán los resultantes de aplicar la formulación del SVM adecuada. 
%En general se podría usar siempre la formulación cuasi-separable, puesto que cuando $\xi_{i}=0$ estamos en el caso de linealmente separable y en caso contrario las clases no se podrían separar linealmente

%\textbf{COMENTARIO: PARA LAS ZONAS EN LAS QUE NO ENCAJE EN NINGUNA DE LAS CLASES PROPUSIERON USAR REGLAS DE PARTIDO DE TENNIS DOS ARTICULOS DISTINTOS (LO HE LEIDO EN Analisys of multiclass support vector machines Abe, Shigeo, SUPONGO QUE FUNCIONA IGUAL PARA OVA )}

\subsection{Directed Acyclic Graph SVM (DAGSVM)}

En este método desarrollado por J.Platt, N.Cristianini y J Shawe-Taylor \cite{article1} se clasifica cada observación $(\textbf{x}_{i},y_{i})$ con $i=1,\dots,n$ para ver en que clase se encuentra, esto se realizará creando un árbol cuyos nodos representarán hiperplanos separadores entre dos clases distintas y los nodos finales corresponderán con cada una de las $k$ clases que poseen las observaciones. Para la elección de los nodos primero es necesario tener una estructura jerárquica de las clases, así estas se podrán ordenar en una lista, y a partir de ella escoger las parejas de clases que corresponderán a cada nodo. Encontrar la forma de estructurar las clases ha sido un campo estudiado ampliamente \cite{6126481}, \cite{DBLP:journals/corr/Panda016},\cite{article6},\cite{article5},  en este trabajo daremos una estructura fija en las clases para su mejor compresión. A continuación se explicará como se eligen las parejas de clases para cada nodo
\begin{itemize}
	\item Nodo Raíz: Para el nodo raíz se escogen las clases que se encuentran en los extremos de la lista, es decir, el primer y último elemento.
	\item Nodos Intermedios: Para facilitar la explicación supongamos que los clases que formaban el hiperplano en el nodo anterior se encontraban en la posición $l$ y $b$ con $l<b$. En esta ocasión existen dos posibilidades
	\begin{itemize}
		\item En el nodo anterior se halla eliminado la clase posicionada más alta en la lista ($b$): En este caso, en el nodo actual tendríamos el hiperplano que separa la clase que no se ha eliminado ($l$) y la enfrentaríamos a la clase que se encuentra en una posición más abajo de la clase eliminada previamente, es decir, $b-1$. 
		\item En el nodo anterior se halla eliminado la clase posicionada más abajo en la lista ($l$):En este caso, en el nodo actual se encontraría el hiperplano que separa la clase que no se ha eliminado($b$) y la clase que se encuentra en una posición más arriba de la clase eliminada previamente, es decir, $l+1$.  
	\end{itemize}
\end{itemize} 

Antes de pasar a un ejemplo veremos como el orden de la lista puede afectar a la elección de la clase para una observación. Supongamos que tenemos el siguiente conjunto de datos 

\begin{figure}[h]	
	\centering
	\subfloat[]{
		
		\includegraphics[width=0.5\textwidth]{DAGSVMpuntoslistas.eps}
		\label{figuraDAGSVMlistas}	
	}
	\subfloat[]{
		
		\includegraphics[width=0.5\textwidth]{DAGSVMejemplolistas.eps}
		\label{figuraDAGSVMlistashiperplanos}
	}
	\caption{La Figura \ref{figuraDAGSVMlistas} muestra el conjunto de observaciones de las que disponemos junto con el ejemplo a estudiar representado por un diamante morado. Por otro lado en \ref{figuraDAGSVMlistashiperplanos} se puede observar las clases con los hiperplanos que separa cada pareja de clases.}
\end{figure}

Primero veamos el caso en el cual el orden jerárquico de las clases es \{0,1,2\}, el árbol tendría la forma de la Figura \ref{figuraDAGSVMlista1}. Con esta lista obtenemos que el ejemplo se encuentra en la clase 2. Sin embargo si tuviéramos la lista \{2,0,1\} el árbol sería el representado en \ref{figuraDAGSVMlista2} y el ejemplo se encontraría en la clase 1. En el siguiente ejemplo se hará un desarrollo más exhaustivo de la elección de la clase a la que pertenecerá una nueva observación.
\begin{figure}[h]	
	\centering
	\subfloat[]{
		
		\includegraphics[width=0.45\textwidth]{DAGSVMlista1.eps}
		\label{figuraDAGSVMlista1}	
	}
	\subfloat[]{
		
		\includegraphics[width=0.45\textwidth]{DAGSVMlista2.eps}
		\label{figuraDAGSVMlista2}
	}
	\caption{La Figura \ref{figuraDAGSVMlista1} se muestra el árbol formado a partir de la lista \{0,1,2\}. Por otro lado en \ref{figuraDAGSVMlista2} se puede observar el árbol desarrollado para la lista \{2,0,1\}.}
\end{figure}
\newpage
Supongamos que disponemos de unas observaciones $(\textbf{x}_{i},y_{i})$ con $\textbf{x}_{i}\in \Omega$ e $y_{i}\in\{0,1,2,3\}$, cuya representación vendría dada por la siguiente imagen.\\
\begin{figure}[h]	
	\centering
	\includegraphics[width=0.65\textwidth]{DAGSVMpuntos.eps}
	\caption{}
	\label{figura34}
	
\end{figure}

Queremos hallar la clase en la que se encuentra nuestra nueva observación denotada en la Figura \ref{figura34} como un diamante morado, para ello se irán escogiendo todas las posibles parejas de clases y descartando clases en las que no se encuentra la nueva observación. Como se ha explicado anteriormente la elección de la pareja se realizará en un proceso arborescente, partiendo de un nodo raíz, una representación de esto sería la dada en la Figura  \ref{figura35}.
\begin{figure}[h]	
	\centering
	\includegraphics[width=0.7\textwidth]{DAGSVM.eps}
	\caption{En la representación mostrada arriba se aprecian las distintas clases y los hiperplanos que separan cada pareja de clases existente. }
	\label{figura35}	
\end{figure}

En el nodo raíz, es decir, el que se encuentra en lo más alto del árbol, se busca un hiperplano que separe la clase 0 de la clase 3, este hiperplano podría ser el representado en la Figura \ref{figura361}, descartamos la clase 3, puesto que el ejemplo no se encuentra en ese lado del hiperplano. Siguiendo el árbol de la figura \ref{figura35} ahora veríamos si el elemento bajo estudio se encuentra en la clase 0 o en la 2, lo que correspondería con la Figura \ref{figura362}

\newpage
\begin{figure}[h]	
	\centering
	\subfloat[]{
		
		\includegraphics[width=0.5\textwidth]{DAGSVMejemplo1.eps}
		\label{figura361}	
	}
	\subfloat[]{
		
		\includegraphics[width=0.5\textwidth]{DAGSVMejemplo2.eps}
		\label{figura362}
	}
	\caption{En la Figura \ref{figura362} se distingue un hiperplano que separa la clase 0 (estrellas), de la clase 3 (círculos). En cambio en \ref{figura361} se puede observar un hiperplano que separa la clase 0 (estrellas) de la clase 2 (triángulos).}
\end{figure}

\hspace{5mm}

\begin{wrapfigure}{r}{0.45\linewidth}
	\begin{center}
		{
			\includegraphics[width=0.48\textwidth]{DAGSVMejemplo3.eps}	
			\caption{Hiperplano de separación entre la clase 0 (estrellas) y la clase 1 (cuadrados).}
			\label{figuraDAGSVM3hiperplano}
		}	
	\end{center}
	
\end{wrapfigure}

Como resultado del hiperplano de la Figura \ref{figura362} descartamos la clase 2 y solo nos quedaría comprobar si pertenece a la clase 0 o a la clase 1, el hiperplano de separación de estas dos clases es el mostrado en \ref{figuraDAGSVM3hiperplano}, y como se puede observar en la imagen, la observación a estudiar pertenece a la clase 0. Con esto hemos llegado al final del árbol representado en \ref{figura35} y podemos concluir que el ejemplo referido como diamante morado pertenece a la clase 0 (estrellas).\\

Hemos visto los tres métodos más usados para agrupar las observaciones que conocemos en dos grupos diferentes y así aplicar el algoritmo SVM, en la siguiente sección se estudiará el método directo, en el que se desarrollarán nuevos modelos basado en el SVM para que diferencia de manera simultánea entre  todas las clases, en lugar de aplicarlo en subgrupos de dos clases.

\section{MÉTODO DIRECTO}\label{MetodoDirecto}

El objetivo de estos métodos es desarrollar un modelo que permita separar las $k$ clases de manera simultánea. En este trabajo estudiaremos principalmente dos modelos, el de Weston-Watkins y Cramer-Singer.

\subsection{Modelo Weston-Watkins}

En esta sección se desarrollará el modelo que fue propuesto simultáneamente por J.Weston y C.Watkins en \cite{Weston-Watkins} y en otro artículo por E.Bredensteiner y K.Bennett  \cite{BENNET}, el análisis se realizará para el caso en que las observaciones sean cuasi-separables, debido a que engloba el caso de linealmente separable y linealmente no separable. En este modelo el clasificador que se utilizará funciona de manera parecida al definido en la Sección \ref{cha:SVM} , pero en lugar de ser un solo hiperplano, será una intersección entre los hiperplanos resultantes de separar una clase en concreto del resto de clases que tendrán nuestras observaciones.\\

 \textbf{COMENTARIO: Quiero poner una imagen, pero no he terminado de programar el algoritmo en Python.}\\

Pasemos a analizar el algoritmo propuesto por Weston-Watkins, primero supongamos que disponemos de un conjunto de observaciones $S=\{(\textbf{x}_{1},y_{1}),\dots,(\textbf{x}_{n},y_{n})\}$ siendo $\textbf{x}_{i}\in\Omega\subset\mathbb{R}^{m}$ e $y_{i}\in Y=\{1,\dots,k\}$, con $i=1,\dots,n$. A diferencia del caso cuasi-separable expuesto en la Sección \ref{subseccioncuasi} , la variable de holgura que definiremos tendrá dos índices, $\xi_{i}^{r}\geq0$ para $i\in\{1,\dots,n\}$ y $r\in \{1,\dots,k\}$ , el contador $i$ esta asociado a la clase de la observación $(\textbf{x}_{i},y_{i})$, por otro lado el contador $r$ corresponde con una de las otras clases que no son $y_{i}$. Para facilitar notación diremos que $\xi_{i}^{y_{i}}=2$, es decir el caso en el que $r=y_{i}$.\\

%\textbf{COMENTARIO: EL CLASIFICADOR ES EL MISMO, SOLO QUE SE USAN DOS HIPERPLANOS, Y LOS \" ENFRENTA", OSEA LO EXPLICO MÁS ADELANTE NO????}

 El objetivo del problema continua siendo maximizar el margen, y por tanto minimizar $\norm{\textbf{v}}_{2}$ o equivalentemente $\frac{1}{2}\norm{\textbf{v}}_{2}^{2}$. A la función objetivo debemos añadir la suma de las variables de holgura que hemos definido, por lo que tendríamos que la función objetivo vendría dada por

%, por otro lado el valor que asociamos a $\xi_{i}^{y_{i}}$ es resultado de aplicar $y_{i}=r$ a la restricción del problema , con lo que obtendríamos $0\geq 2-\xi_{i}^{y_{i}}$ por lo que para que tenga sentido vamos a tomar el menor valor posible de $\xi_{i}^{y_{i}}$ es decir 2

\begin{equation}\label{funcionobjetivoWW}
\text{min} \quad \frac{1}{2}\norm{\textbf{v}}_{2}^{2} + C\sum_{i=1}^{n}\sum_{r=1}^{k}\xi_{i}^{r}.
\end{equation}

Veamos cual sería la restricción para este problema. Para mejor comprensión se empezará estudiando el caso en el que solo existan dos clases y lo generalizamos posteriormente a $k$ clases. Supongamos que nuestras observaciones poseen únicamente dos clases que denotaremos como $i,j$, por ende la observación solo podrá encontrarse en un lado del hiperplano, es decir,
\begin{equation}\label{restriccionWW2clases1}\nonumber
<\textbf{v},\textbf{x}_{i}>+a_{i}\geq 1-\xi \quad \text{si} \quad y_{i}=i,
\end{equation}
\begin{equation}\label{restriccionWW2clases2}\nonumber
<\textbf{v},\textbf{x}_{i}>+a_{j}\leq \xi-1 \quad \text{si} \quad y_{i}=j.
\end{equation}

Las expresiones anteriores se pueden deducir a partir de la ecuación \eqref{funcionobjetivomincuasi} . En el caso que nos atañe, queremos ver si la observación está en su clase asociada $y_{i}$, o en otra de las clases existentes. Tras generalizar obtenemos dos expresiones 
\begin{equation}\label{restriccionWWkclases1}\nonumber
<\textbf{v}_{y_{i}},\textbf{x}_{i}>+a_{y_{i}}\geq 1-\xi_{i} \quad \text{si} \quad y_{i}=i,
\end{equation}
\begin{equation}\label{restriccionWWkclases2}\nonumber
<\textbf{v}_{r},\textbf{x}_{i}>+a_{r}\leq \xi_{r}-1 \quad \text{si} \quad y_{i}=r,
\end{equation}

la variable $\textbf{v}_{y_{i}}$ representa el hiperplano que separa la clase $y_{i}$ del resto de clases y la variable $\textbf{v}_{r}$ el hiperplano que separa la clase $r$ del resto de las clases existentes.\\

Para agrupar las dos expresiones anteriores restamos ambas desigualdades, obteniendo 
\begin{equation}\label{restriccionWW}
<\textbf{v}_{y_{i}},\textbf{x}_{i}>+a_{y_{i}}-<\textbf{v}_{r},\textbf{x}_{i}>-a_{r}\geq 2-\xi_{i}^{r},
\end{equation}
donde $\xi_{i}^{r}$ corresponde con la suma de las expresiones $\xi_{i},\xi_{r}$ que son las variables de holgura los hiperplanos que separan la clase $y_{i}, r$ respectivamente del resto. Con la expresión (\ref{restriccionWW}) sí se tienen en cuenta todas las clases existentes en el grupo de observaciones y se realizaría un total de $nk$ veces, siendo $n$ el número de observaciones y $k$ el número de clases totales. El problema a resolver sería el siguiente 
\begin{alignat*}{3}
 \quad & \text{min}  \quad && \frac{1}{2}\sum_{r=1}^{k}\norm{\textbf{v}_{r}}_{2}^{2} + C\sum_{i=1}^{n}\sum_{r=1}^{k} \xi_{i}^{r}\qquad&& \\
& \text{s.a:}   \quad && <\textbf{v}_{y_{i}},\textbf{x}_{i}>+a_{y_{i}}-<\textbf{v}_{r},\textbf{x}_{i}>-a_{r}\geq 2-\xi_{i}^{r}, \quad  i=1,\dots,n, \quad r\in\{1,\dots,k\} && \\
&\qquad && \xi_{i}^{r}\geq 0.
\end{alignat*}

Como ya hemos explicamos anteriormente, resolveremos el problema dual usando la relajación Lagrangiana, para ello se necesitará conocer cual es su función de Lagrange, teniendo esta la siguiente expresión
\begin{equation}\label{LagrangeWW}
\begin{split}
L(\textbf{v}_{r},a_{r},\boldsymbol{\xi},\boldsymbol{\alpha},\boldsymbol{\beta})&=\frac{1}{2}\sum_{r=1}^{k}\norm{\textbf{v}_{r}}_{2}^{2}+C\sum_{i=1}^{n}\sum_{r=1}^{k} \xi_{i}^{r}-\sum_{i=1}^{n}\sum_{r=1}^{k}\beta_{i}^{r}\xi_{i}^{r}-\\
&-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}[<\textbf{v}_{y_{i}},\textbf{x}_{i}>+a_{y_{i}}-<\textbf{v}_{r},\textbf{x}_{i}>-a_{r}-2+\xi_{i}^{r}],\\
\end{split}
\end{equation}
en este caso, los multiplicadores de Lagrange serán $\alpha_{i}^{r}\geq0$, $\beta_{i}^{r}\geq0$ para $i=1,\dots,n$ y $r\in\{1,\dots k\} \backslash y_{i}$. En los casos en que $y_{i}=r$, es decir los casos en que queremos separar la clase $y_{i}$ de la clase $y_{i}$, se impondrán los siguientes valores 
\begin{equation}\label{condicionesclasesWW}
\alpha_{i}^{y_{i}}=0, \quad \beta_{i}^{y_{i}}=0, \quad i=1,\dots, n,
\end{equation}
estos valores son consecuencia de que es debido a que es fútil separar una clase de sí misma.\\

A continuación aplicamos el Teorema KKT \ref{teorema312} , en la primera de las condiciones de este teorema se usan las derivadas parciales, en este caso respecto a las variables $\textbf{v}_{r}, a_{r}$ y $\xi_{i}^{r}$. Como en la expresión de la función de Lagrange \eqref{LagrangeWW} tenemos las variables $\textbf{v}_{y_{i}},a_{y_{i}}$ introducimos unas variables con las que consideraremos si $y_{i}=r$, y por tanto fuera necesario derivar también $\textbf{v}_{y_{i}}$ y $a_{y_{i}}$ 
\begin{equation}\nonumber
A_{i}=\sum_{r=1}^{k}\alpha_{i}^{r}, \quad c_{i}^{r}=\left\lbrace\begin{array}{c} 1~	\text{si}~y_{i}=r \\ 0~\text{si}~y_{i}\neq r \end{array}\right.
\end{equation}


Procedemos a realizar los cálculos de la primera de las condiciones del Teorema KKT (\ref{condicion1})
\begin{equation}\label{derivada1WW}
\frac{\partial L(\textbf{v}_{r},a_{r},\boldsymbol{\xi},\boldsymbol{\alpha},\boldsymbol{\beta})}{\partial \textbf{v}_{r}}= \textbf{v}_{r}+\sum_{i=1}^{n}\alpha_{i}^{r}\textbf{x}_{i} - \sum_{i=1}^{n}A_{i}c_{i}^{r}\textbf{x}_{i}=0, \qquad i=1,\dots,n,
\end{equation}
\begin{equation}\label{derivada2WW}
\frac{\partial L(\textbf{v}_{r},a_{r},\boldsymbol{\xi},\boldsymbol{\alpha},\boldsymbol{\beta})}{\partial a_{r}}= -\sum_{i=1}^{n}A_{i}c_{i}^{r}+\sum_{i=1}^{n}\alpha_{i}^{r}=0, \qquad i=1,\dots,n,
\end{equation}
\begin{equation}\label{derivada3WW}
\frac{\partial L(\textbf{v}_{r},a_{r},\boldsymbol{\xi},\boldsymbol{\alpha},\boldsymbol{\beta})}{\partial \xi_{i}^{r}}= -\alpha_{i}^{r}+C-\beta_{i}^{r}=0, \qquad i=1,\dots,n.
\end{equation}

De las derivadas anteriores se pueden deducir las siguientes expresiones
\begin{equation}\label{consecuencia1WW}
\textbf{v}_{r}=\sum_{i=1}^{n}A_{i}c_{i}^{r}\textbf{x}_{i}-\sum_{i=1}^{n}\alpha_{i}^{r}, \qquad i=1,\dots,n,
\end{equation}
\begin{equation}\label{consecuencia2WW}
\sum_{i=1}^{n}A_{i}c_{i}^{r}=\sum_{i=1}^{n}\alpha_{i}^{r}, \qquad i=1,\dots,n,
\end{equation}
\begin{equation}\label{consecuencia3WW}
C=\alpha_{i}^{r}+\beta{i}^{r} \qquad i=1,\dots,n.
\end{equation}

De la expresión \eqref{consecuencia3WW} se puede obtener la siguiente acotación para $\alpha_{i}^{r}$
$$0\leq\alpha_{i}^{r}\leq C, $$
debido a que $\beta_{i}^{r}\geq0, \alpha_{i}^{r}\geq0$ para $i=1,\dots,n$  y $r\in\{1,\dots, k\}$. Sustituimos la expresión \eqref{consecuencia1WW} y \eqref{consecuencia3WW} en \eqref{LagrangeWW}
%(LE QUITO $y_{i}$???)
\begin{equation}\label{sustitucionWW}
\begin{split}
L(\textbf{v}_{r},a_{r},\boldsymbol{\xi},\boldsymbol{\alpha},\boldsymbol{\beta})&\footnotesize{=\frac{1}{2}\sum_{r=1}^{k}\sum_{i=1}^{n}\sum_{j=1}^{n}(c_{i}^{r}A_{i}-\alpha_{i}^{r})(c_{j}^{r}A_{j}-\alpha_{j}^{r})<\textbf{x}_{i},\textbf{x}_{j}>+C\sum_{i=1}^{n}\sum_{r=1}^{k}\xi_{i}^{r}-}\\
&\footnotesize{\hspace{5mm}-\sum_{i=1}^{n}\sum_{r=1}^{k}\beta_{i}^{r}\xi_{i}^{r}-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\xi_{i}^{r}-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\Bigg[\sum_{j=1}^{n}(c_{j}^{y_{i}}A_{j}-\alpha_{j}^{y_{i}})<\textbf{x}_{i},\textbf{x}_{j}>-}\\
&\footnotesize{\hspace{5mm}-a_{r}-\sum_{j=1}^{n}(c_{j}^{r}A_{j}-\alpha_{j}^{r})<\textbf{x}_{i},\textbf{x}_{j}>+a_{y_{i}}-2\Bigg]=}\\
&\footnotesize{=-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{k}\Bigg[\sum_{j=1}^{n}(c_{j}^{y_{i}}A_{j}-\alpha_{j}^{y_{i}})<\textbf{x}_{i},\textbf{x}_{j}>-\sum_{j=1}^{n}(c_{j}^{r}A_{j}-\alpha_{j}^{r})<\textbf{x}_{i},\textbf{x}_{j}>+}\\
& \footnotesize{\hspace{5mm}+a_{y_{i}}-a_{r}-2\Bigg]+ \frac{1}{2}\sum_{r=1}^{k}\sum_{i=1}^{n}\sum_{j=1}^{n}(c_{i}^{r}A_{i}-\alpha_{i}^{r})(c_{j}^{r}A_{j}-\alpha_{j}^{r})<\textbf{x}_{i},\textbf{x}_{j}>.}
\end{split}
\end{equation}

Antes de continuar con el desarrollo de la función de Lagrange veamos que se verifica la siguiente igualdad 
\begin{equation}\label{igualdada}
\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}a_{y_{i}}=\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}a_{r},
\end{equation} 
para ello, desarrollaremos primero el lado izquierdo de la igualdad. Lo que se busca es transformar la variable $a_{y_{i}}$ en $a_{m}$, es decir, en lugar de considerar el término independiente del hiperplano asociado a cada observación, solo consideraremos el hiperplano asociado a cada clase. Se puede observar que por la definición dada previamente de la variable $c_{i}^{r}$ se verifica la siguiente igualdad
\begin{equation}\label{clasesigualobsevaciones}\nonumber
\sum_{i=1}^{n}\sum_{r=1}^{k}c_{i}^{r}a_{y_{i}}=\sum_{r=1}^{k}a_{r},
\end{equation}
por tanto mediante la expresión anterior y con el uso de la definición de $A_{i}$ se puede ver que se verifican las siguientes igualdades
\begin{equation}\label{expresionb1WW}
\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}a_{y_{i}}=\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}a_{r}c_{i}^{r}=\sum_{r=1}^{k}a_{r}\sum_{i=1}^{n}c_{i}^{r}\alpha_{i}^{r}=\sum_{r=1}^{k}a_{r}\sum_{i=1}^{n}c_{i}^{r}A_{i}.
\end{equation}

Por otro lado desarrollamos el lado derecho de la igualdad \eqref{igualdada}
\begin{equation}\label{expresionb2WW}
\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}a_{r}=\sum_{r=1}^{k}a_{r}\sum_{i=1}^{n}\alpha_{i}^{r}.
\end{equation}

Aplicando la expresión \eqref{consecuencia2WW} confirmamos que las dos expresiones anteriores, \eqref{expresionb1WW} y \eqref{expresionb2WW} son iguales, y por tanto al estar restándose en la función de Lagrange se anulan. Continuamos desarrollando la función de Lagrange \ref{sustitucionWW}
\begin{equation}\label{sustitucion1WW}\nonumber
\begin{split}
\small{L(\textbf{v}_{r},a_{r},\boldsymbol{\xi},\boldsymbol{\alpha},\boldsymbol{\beta})}&\footnotesize{=-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\Bigg[\sum_{j=1}^{n}(c_{j}^{y_{i}}A_{j}-\alpha_{j}^{y_{i}})<\textbf{x}_{i},\textbf{x}_{j}>-\sum_{j=1}^{n}(c_{j}^{r}A_{j}-\alpha_{j}^{r})<\textbf{x}_{i},\textbf{x}_{j}>+}\\
& \footnotesize{\hspace{5mm}+a_{y_{i}}-a_{r}-2\Bigg]+ \frac{1}{2}\sum_{r=1}^{k}\left(\sum_{i=1}^{n}(c_{i}^{r}A_{i}-\alpha_{i}^{r})\textbf{x}_{i}\right)\left(\sum_{j=1}^{n}(c_{j}^{r}A_{j}-\alpha_{j}^{r})\textbf{x}_{j}\right)=}\\
&\footnotesize{=-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\Bigg[\sum_{j=1}^{n}(c_{j}^{y_{i}}A_{j}-\alpha_{j}^{y_{i}})<\textbf{x}_{i},\textbf{x}_{j}>-\sum_{j=1}^{n}(c_{j}^{r}A_{j}-\alpha_{j}^{r})<\textbf{x}_{i},\textbf{x}_{j}>-}\\
&\footnotesize{\hspace{5mm}-2\Bigg]+ \frac{1}{2}\sum_{r=1}^{k}\left(\sum_{i=1}^{n}(c_{i}^{r}A_{i}-\alpha_{i}^{r})\textbf{x}_{i}\right)\left(\sum_{j=1}^{n}(c_{j}^{r}A_{j}-\alpha_{j}^{r})\textbf{x}_{j}\right)=}\\
&\footnotesize{=-\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{r=1}^{k}\left[\alpha_{i}^{r}c_{i}^{y_{i}}A_{j}-\alpha_{i}^{r}\alpha_{j}^{y_{i}}-\alpha_{i}^{r}c_{j}^{r}A_{j}+\alpha_{i}^{r}\alpha_{j}^{r}\right]<\textbf{x}_{i},\textbf{x}_{j}>+2\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}+}\\
&\footnotesize{\hspace{5mm}+\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{r=1}^{k}\left[c_{i}^{r}c_{j}^{r}A_{i}A_{j}-\alpha_{j}^{r}c_{i}^{r}A_{i}-\alpha_{i}^{r}c_{j}^{r}A_{j}+\alpha_{i}^{r}\alpha_{j}^{r}\right]<\textbf{x}_{i},\textbf{x}_{j}>=}\\
&\footnotesize{=2\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}+\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{r=1}^{k}\Bigg[\frac{1}{2}c_{i}^{r}c_{j}^{r}A_{i}A_{j}-\frac{1}{2}\alpha_{j}^{r}c_{i}^{r}A_{i}-\frac{1}{2}\alpha_{i}^{r}c_{j}^{r}A_{j}+\frac{1}{2}\alpha_{i}^{r}\alpha_{j}^{r}-}\\
&\footnotesize{\hspace{5mm}-\alpha_{i}^{r}c_{i}^{y_{i}}A_{j}+\alpha_{i}^{r}\alpha_{j}^{y_{i}}+\alpha_{i}^{r}c_{j}^{r}A_{j}-\alpha_{i}^{r}\alpha_{j}^{r}\Bigg]<\textbf{x}_{i},\textbf{x}_{j}>.}
\end{split}
\end{equation}

A continuación empleamos en la ecuación anterior la expresión $\sum_{r}c_{i}^{r}A_{i}\alpha_{j}^{r}=\sum_{r}\alpha_{j}^{r}A_{j}\alpha_{i}^{r}$, esto se verifica debido a que 
\begin{equation}\label{igualdadWW}\nonumber
\sum_{r=1}^{k}c_{i}^{r}A_{i}\alpha_{j}^{r}=\sum_{r=1}^{k}c_{i}^{r}\alpha_{i}^{r}\alpha_{j}^{r}=\sum_{r=1}^{k}c_{j}^{r}\alpha_{i}^{r}\alpha_{j}^{r}=\sum_{r=1}^{k}c_{j}^{r}A_{j}\alpha_{i}^{r},
\end{equation}
con lo que obtendríamos

\begin{equation}\label{sustitucion2WW}\nonumber
\begin{split}
L(\textbf{v}_{r},a_{r},\boldsymbol{\xi},\boldsymbol{\alpha},\boldsymbol{\beta})&=2\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}+\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{r=1}^{k}\Bigg[\frac{1}{2}c_{i}^{r}c_{j}^{r}A_{i}A_{j}-\frac{1}{2}\alpha_{j}^{r}c_{i}^{r}A_{i}-\frac{1}{2}\alpha_{i}^{r}c_{j}^{r}A_{j}+\\
&\hspace{5mm}+\frac{1}{2}\alpha_{i}^{r}\alpha_{j}^{r}-\alpha_{i}^{r}\alpha_{j}^{r}-\alpha_{i}^{r}c_{i}^{y_{i}}A_{j}+\alpha_{i}^{r}\alpha_{j}^{y_{i}}+\alpha_{i}^{r}c_{j}^{r}A_{j}\Bigg]<\textbf{x}_{i},\textbf{x}_{j}>=\\
&=2\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}+\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{r=1}^{k}\Bigg[\frac{1}{2}c_{i}^{r}c_{j}^{r}A_{i}A_{j}-\frac{1}{2}c_{i}^{r}A_{i}\alpha_{j}^{r}-\frac{1}{2}c_{j}^{r}A_{j}\alpha_{i}^{r}+\\
&\hspace{5mm}+c_{j}^{r}A_{j}\alpha_{i}^{r}-c_{j}^{y_{i}}A_{j}\alpha_{i}^{r}+\alpha_{i}^{r}\alpha_{j}^{y_{i}}-\frac{1}{2}\alpha_{i}^{r}\alpha_{j}^{r}\Bigg]<\textbf{x}_{i},\textbf{x}_{j}>=\\
&=2\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}+\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{r=1}^{k}\Bigg[\frac{1}{2}c_{i}^{r}c_{j}^{r}A_{i}A_{j}-\frac{1}{2}c_{i}^{r}A_{i}\alpha_{j}^{r}+\frac{1}{2}c_{j}^{r}A_{j}\alpha_{i}^{r}-\\
&\hspace{5mm}-c_{j}^{y_{i}}A_{j}\alpha_{i}^{r}+\alpha_{i}^{r}\alpha_{j}^{y_{i}}-\frac{1}{2}\alpha_{i}^{r}\alpha_{j}^{r}\Bigg]<\textbf{x}_{i},\textbf{x}_{j}>=\\
&=+\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{r=1}^{k}\left[\frac{1}{2}c_{i}^{r}c_{j}^{r}A_{i}A_{j}-c_{j}^{y_{i}}A_{j}\alpha_{i}^{r}+\alpha_{i}^{r}\alpha_{j}^{y_{i}}-\frac{1}{2}\alpha_{i}^{r}\alpha_{j}^{r}\right]<\textbf{x}_{i},\textbf{x}_{j}>+\\
&\hspace{5mm}+2\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}.
\end{split}
\end{equation}

Por tanto el problema a resolver sería el siguiente 
\begin{alignat*}{3}
\textbf{(PWW)} \quad & \text{min}  \quad && 2\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}+\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{r=1}^{k}\left[\frac{1}{2}c_{i}^{r}c_{j}^{r}A_{i}A_{j}-c_{j}^{y_{i}}A_{j}\alpha_{i}^{r}+\alpha_{i}^{r}\alpha_{j}^{y_{i}}-\frac{1}{2}\alpha_{i}^{r}\alpha_{j}^{r}\right]<\textbf{x}_{i},\textbf{x}_{j}>\qquad&& \\
& \text{s.a:}   \quad && \sum_{i=1}^{n}\alpha_{i}^{r}=\sum_{i=1}^{n}A_{i}c_{i}^{r}, \quad && \\
&\qquad && \xi_{i}^{r}\geq 0,\quad 0\leq\alpha_{i}^{r}\leq C,\quad i=1,\dots,n, \quad r\in\{1,\dots,k\} \backslash y_{i}.
\end{alignat*}

Este modelo no es el único existente, como explicamos a principio de la Sección \ref{MetodoDirecto} en este trabajo se desarrollará el modelo propuesto por Cramer-Singer que estudiaremos a continuación.
\newpage

\subsection{Modelo Crammer-Singer}

Otro modelo para clasificar las $k$ clases simultáneamente es el realizado por K.Crammer y Y.Singer \cite{article2}, en este partimos de un conjunto de observaciones $S=\{(\textbf{x}_{1},y_{1}),\dots,(\textbf{x}_{n},y_{n})\}$ siendo $\textbf{x}_{i}\in\Omega\subset\mathbb{R}^{m}$ e $y_{i}\in Y=\{1,\dots,k\}$, con $i=1,\dots,n$, para explicar esta formulación pasaremos a desarrollar el clasificador que vamos a utilizar en esta sección.\\  


%\begin{definicion}[label={normadual},nameref={Title or anything else}]{Clasificador}
%	Sea $\{(\textbf{x}_{1},y_{1}),\dots,(\textbf{x}_{n},y_{n})\}$ un conjunto de observaciones con $\textbf{x}_{i}\in\Omega\subset\mathbb{R}^{m}$ e  $y_{i}\in Y=\{1,\dots,k\}$ para $i=1,\dots,n$. Sea M una matriz $k\times m$ cuyas filas equivaldrían al valor de y$\cdot$\textbf{v}, con \textbf{v} el vector normal del hiperplano H dado en la Definición \ref{hiperplano}\space. Un clasificador es una función $C_{\textbf{M}}:\Omega\subset\mathbb{R}^{n} \rightarrow Y$ que asocia un valor $y$ a la variable $\textbf{x}_{i}$, la expresión del clasificador viene dada por
%	$$C_{M}(\textbf{x}_{i})=arg \max_{r=1}^{k}\{M_{r}\cdot \textbf{x}_{i}\},$$
%	 siendo $M_{r}$ la fila r-ésima de \textbf{M}.
%\end{definicion}

Defininmos una matriz M de dimensión $k\times m$ cuyas filas equivaldrían al valor de $y \cdot\textbf{v}$, con \textbf{v} el vector normal del hiperplano H dado en la Definición \ref{hiperplano}\space y cada columna correspondería con una de las k clases existentes. El clasificador que aplicaremos es una función $C_{\textbf{M}}:\Omega\subset\mathbb{R}^{n} \rightarrow Y$ que asocia un valor $y$ a la variable $\textbf{x}_{i}$, la expresión del clasificador vendrá dada por
$$C_{M}(\textbf{x}_{i})=arg \stackbin[r=1]{k}{\text{max}}\{M_{r}\cdot \textbf{x}_{i}\}, \quad \text{siendo $M_{r}$ la fila r-ésima de \textbf{M}.}$$

Decimos que $(\textbf{x}_{i},y_{i})$ está mal clasificado cuando $C_{M}(\textbf{x}_{i})\neq y_{i}$. Definimos $[\pi]$ como 1 cuando $\pi$ es cierto y 0 en caso contrario. Por tanto el error empírico de un problema mutliclase es la cantidad de veces que una observación está mal clasificada dividido por el número total de observaciones, es decir, 

\begin{equation}\label{errorempirico1}
\epsilon_{S}(M)=\frac{1}{n}\sum_{i=1}^{n}[C_{M}(\textbf{x}_{i})\neq y_{i}]. 
\end{equation}

La definición que se ha dado de clasificador es buena cuando el número total de clases es igual a dos, pero cuando este es mayor o iguala a tres es menos eficiente, por ello para aplicar el clasificador a un problema multiclase reemplazamos $arg \stackbin[r=1]{k}{\text{max}}\{M_{r}\cdot \textbf{x}_{i}\}$ por 
\begin{equation}\label{cambioerror}
\max_{r}\{M_{r}\textbf{x}_{i}+1-\delta_{y_{i},r}\}-M_{y_{i}}\textbf{x}_{i},
\end{equation}
 donde $\delta_{p,d}$ es un parámetro cuyo valor es 1 si $p=d$ y 0 en caso contrario. La expresión \eqref{cambioerror} es 0 cuando $M_{y_{i}}\textbf{x}_{i}\geq M_{r}\textbf{x}_{i}+1$, para todo $r\in\{1,\dots,k\}\backslash y_{i}$, esto ocurre cuando $(\textbf{x}_{i},y_{i})$ se encuentra bien clasificado. Por otro lado, cuando el máximo que se busca no coincide con la clase $y_{i}$, nos encontramos dos situaciones\\
 
\textbf{COMENTARIO: Las desigualdades de abajo no son realmente así ¿no?, sería con $max\{M_{r}x_{i}\}$. Me baso en el documento de Cramer-Singer pag 4, ultimo párrafo. Por otro lado quiero poner unas imágenes en lugar de las usar las figuras 2.5a, 2.5b de la sección anterior, pero antes de añadirles necesito saber si es correcto lo que os estoy preguntando.}

 \begin{itemize}
 	\item $M_{r}\textbf{x}_{i}-M_{y_{i}}\textbf{x}_{i}\leq0$ en cuyo caso $(\textbf{x}_{i},y_{i})$ está bien clasificado, pero se encuentra fuera del hiperespacio adecuado, como se muestra en la Figura (PONER IMAGEN) %\ref{figura241} .
 	\item $M_{r}\textbf{x}_{i}-M_{y_{i}}\textbf{x}_{i}\geq0$ en esta situación $(\textbf{x}_{i},y_{i})$ se encuentra mal clasificado, como se muestra en la Figura (PONER IMAGEN)%\ref{figura242} .
 \end{itemize}
 
 Con la expresión \eqref{cambioerror} se puede obtener una cota superior del error empírico, siendo esta
 \begin{equation}\label{errorempirico2}\nonumber
\epsilon_{S}(M)\leq\frac{1}{n}\sum_{i=1}^{n}[\stackbin[r]{}{\text{max}} \{M_{r}\textbf{x}_{i}+1-\delta_{y_{i},r}\}-M_{y_{i}}\textbf{x}_{i}].
 \end{equation}
 
 Mediante esta cota superior del error empírico se puede ver si las observaciones son linealmente separables, cuasi-separables o linealmente no separables. En este trabajo estudiaremos el caso cuasi-separable, puesto que es una generalización de los casos linealmente separables y linealmente no separables.\\
 
 La cota superior \eqref{cambioerror} verifica que si las observaciones son cuasi-separables entonces $\max_{r}\{M_{r}\textbf{x}_{i}+1-\delta_{y_{i},r}\}-M_{y_{i}}\textbf{x}_{i}=\xi_{i}$ para $i\in\{1,\dots,n\}$, con $\xi_{i}\geq0$ una variable de holgura, puesto que existirá un error de clasificación asociado a cada observación. De esta expresión se deduce que $M_{y_{i}}\textbf{x}_{i}-M_{r}\textbf{x}_{i}+\delta_{y_{i},r}\geq 1-\xi_{i}$.\\
 
 Por tanto la expresión del problema sería la siguiente
 \begin{alignat*}{3}
 \textbf{(PCS1)} \quad & \text{min}  \quad && \frac{1}{2}\beta\sum_{r=1}^{k}\norm{M_{r}}_{2}^{2} + \sum_{i=1}^{n}\xi_{i}\qquad&& \\
 & \text{s.a:}   \quad && M_{y_{i}}\textbf{x}_{i}-M_{r}\textbf{x}_{i}+\delta_{y_{i},r}\geq 1-\xi_{i}, \quad && \\
 &\qquad &&  i=1,\dots,n, \quad r\in\{1,\dots,k\},
 \end{alignat*}
 siendo $\beta\in\mathbb{R}$ una constante reguladora entre la matriz $\textbf{M}$ y la variable de holgura $\boldsymbol{\xi}$.\\
 
 En el problema anterior no encontramos a simple vista la restricción $\xi_{i}\geq0$, pero debido a que este problema se define para cada $i=1,\dots,n$ y $r=\in\{1,\dots,k\}$, el caso en el que $i=r$ la restricción tiene la siguiente expresión
  \begin{equation}\nonumber
M_{y_{i}}\textbf{x}_{i}-M_{y_{i}}\textbf{x}_{i}+1\geq1-\xi_{i}\qquad \text{o lo que es lo mismo} \qquad \xi_{i}\geq 0.
 \end{equation}
 
 
 Tras esta puntualización aplicamos la relajación Lagrangiana, por lo que necesitamos conocer la expresión de su función de Lagrange, que es la siguiente
 \begin{equation}\label{lagrangeCS}
 L(M_{r},\boldsymbol{\xi},\boldsymbol{\alpha})=\frac{1}{2}\beta\sum_{r=1}^{k}\norm{M_{r}}_{2}^{2} + \sum_{i=1}^{n}\xi_{i} -\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}(
 M_{y_{i}}\textbf{x}_{i}-M_{r}\textbf{x}_{i}+\delta_{y_{i},r}-1+\xi_{i}),\\
 \end{equation}
con $\alpha_{i}^{r}\geq0$ para $i=1,\dots,n$ y $r=1,\dots,k$ los multiplicadores de lagrange.\\

Empleando un razonamiento similar al usado en la Sección \ref{SVMLS} aplicamos el Teorema de Karush-Kuhn-Tucker \ref{teorema312} , para ello realizamos las derivadas parciales de la función de Lagrange con respecto a las variables $\boldsymbol{\xi},\textbf{M}$.



\begin{equation}\label{condicion1CS}
\begin{split}
& \frac{\partial L(M_{r},\boldsymbol{\xi},\boldsymbol{\alpha})}{\partial \xi_{i}} =1-\sum_{r=1}^{k}\alpha_{i}^{r}=0,  \qquad i=1,\dots,n,  \\
\end{split}
\end{equation}
 
\begin{equation}\label{condicion2CS}
\begin{split}
& \frac{\partial L(M_{r},\boldsymbol{\xi},\boldsymbol{\alpha})}{\partial M_{r}} =\beta M_{r}+\sum_{i=1}^{n}\alpha_{i}^{r}\textbf{x}_{i}-\sum_{i=1,y_{i}=r}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\textbf{x}_{i}=0,  \qquad r=1,\dots,k.  \\
\end{split}
\end{equation}

%\textbf{LA SEGUNDA DE LAS CONDICIONES DEL TEOREMA KKT NORMALMENTE SE APLICA PARA HALLAR EL HIPERPLANO, LO VOY A HALLAR DESPUÉS.}\\

Desarrollemos las expresiones anteriores, de la primera de ellas se puede deducir que 
\begin{equation}\label{conclusioncondicion1CS}
\sum_{r=1}^{k}\alpha_{i}^{r}=1, \quad i=1\dots,n.
\end{equation}

De la expresión \eqref{condicion2CS} deducimos 
\begin{equation}\nonumber
\begin{split}
&\beta M_{r}+\sum_{i=1}^{n}\alpha_{i}^{r}\textbf{x}_{i}-\sum_{i=1,y_{i}=r}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\textbf{x}_{i}=\beta M_{r}+\sum_{i=1}^{n}\alpha_{i}^{r}\textbf{x}_{i}-\sum_{i=1,y_{i}=r}^{n}\left(\sum_{r=1}^{k}\alpha_{i}^{r}\right)\textbf{x}_{i}=\\
&=\beta M_{r}+\sum_{i=1}^{n}\alpha_{i}^{r}\textbf{x}_{i}-\sum_{i=1,y_{i}=r}^{n}\textbf{x}_{i}=\beta M_{r}+\sum_{i=1}^{n}\alpha_{i}^{r}\textbf{x}_{i}-\sum_{i=1}^{n}\delta_{y_{i},r}\textbf{x}_{i}=0, \quad i=1\dots,n,
\end{split}
\end{equation}
donde despejando la variable $M_{r}$ obtenemos 
\begin{equation}\label{conclusioncondicion2CS}
M_{r}=\frac{1}{\beta}\left(\sum_{i=1}^{n}(\delta_{i,r}-\alpha_{i}^{r})\textbf{x}_{i}\right), \quad r=1\dots,k.
\end{equation}

%\textbf{COMENTARIO: AQUÍ PONE UNA EXPLICACIÓN DE LOS VECTORES SOPORTE, PERO NO TENGO CLARO SI QUIERO AÑADIRLO.}\\
A continuación sustituimos las expresiones \eqref{conclusioncondicion1CS} y \eqref{conclusioncondicion2CS} en \eqref{lagrangeCS} obteniendo\\

\begin{equation}\label{lagrangedesarrolloCS}\nonumber
\begin{split}
L(M_{r},\boldsymbol{\xi},\boldsymbol{\alpha})&=\frac{1}{2}\beta\sum_{r=1}^{k}\norm{M_{r}}_{2}^{2}+\sum_{i=1}^{n}\xi_{i}-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}M_{y_{i}}\textbf{x}_{i}+\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}M_{r}\textbf{x}_{i}-\\
&\hspace{5mm} -\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\delta_{y_{i},r}+\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\xi_{i}=\\
&=\frac{1}{2}\beta\sum_{i=1}^{k}\norm{M_{r}}_{2}^{2}+\sum_{i=1}^{n}\xi_{i}-\sum_{i=1}^{n}\xi_{i}-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}M_{y_{i}}\textbf{x}_{i}+\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}M_{r}\textbf{x}_{i}+\\
&\hspace{5mm}+\sum_{i=1}^{n}1-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\delta_{y_{i},r}=\\
&=\underbrace{\frac{1}{2}\beta\sum_{r=1}^{k}\norm{M_{r}}_{2}^{2}}_{\text{($S_{1}$)}}-\underbrace{\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}M_{y_{i}}\textbf{x}_{i}}_{\text{($S_{2}$)}}+\underbrace{\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}M_{r}\textbf{x}_{i}}_{\text{($S_{3}$})}-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\delta_{y_{i},r}+n.\\
\end{split}
\end{equation}

Tras el desarrollo anterior, continuaremos con la explicación de cada uno de los grupos que se han definido como $(S_{1}),(S_{2}),(S_{3})$. Comenzamos con $(S_{1})$, sustituimos la expresión \eqref{conclusioncondicion2CS} 
\begin{equation}\label{S1}\nonumber
\begin{split}
(S_{1})&=\frac{1}{2}\beta\sum_{r=1}^{k}<M_{r},M_{r}>=\frac{1}{2}\beta\sum_{r=1}^{k}\Bigg[\frac{1}{\beta}\sum_{i=1}^{n}(\delta_{y_{i},r}-\alpha_{i}^{r})\textbf{x}_{i}\Bigg]\Bigg[\frac{1}{\beta}\sum_{j=1}^{n}(\delta_{y_{j},r}-\alpha_{j}^{r})\textbf{x}_{j}\Bigg]=\\
&=\frac{1}{2\beta}\sum_{i=1}^{n}\sum_{j=1}^{n}<\textbf{x}_{i},\textbf{x}_{j}>\sum_{r=1}^{k}(\delta_{y_{i},r}-\alpha_{i}^{r})(\delta_{y_{j},r}-\alpha_{j}^{r}).\\
\end{split}
\end{equation}

Tras obtener la expresión $(S_{1})$, pasaremos a hacer lo mismo con $(S_{2})$ aplicando en su desarrollo la definición de la variable $\delta_{i,r}$, esto es, $\delta_{i,r}=1$ si $i=r$ y $\delta_{i,r}=0$ en caso contrario.
\begin{equation}\label{S2}\nonumber
\begin{split}
(S_{2})&=\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}M_{y_{i}}\textbf{x}_{i}=\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\textbf{x}_{i}\frac{1}{\beta}\sum_{j=1}^{n}(\delta_{y_{j},y_{i}}-\alpha_{y_{j}}^{y_{i}})\textbf{x}_{j}=\\
&=\frac{1}{\beta}\sum_{i=1}^{n}\sum_{j=1}^{n}<\textbf{x}_{i},\textbf{x}_{j}>(\delta_{y_{j},y_{i}}-\alpha_{y_{j}}^{y_{i}})\sum_{r=1}^{k}\alpha_{i}^{r}=\frac{1}{\beta}\sum_{i=1}^{n}\sum_{j=1}^{n}<\textbf{x}_{i},\textbf{x}_{j}>(\delta_{y_{j},y_{i}}-\alpha_{y_{j}}^{y_{i}})=\\
&=\frac{1}{\beta}\sum_{i=1}^{n}\sum_{j=1}^{n}<\textbf{x}_{i},\textbf{x}_{j}>\sum_{r=1}^{k}\delta_{y_{i},r}(\delta_{y_{j},r}-\alpha_{y_{j}}^{r}).
\end{split}
\end{equation}

Por último desarrollamos $(S_{3})$
\begin{equation}\label{S3}\nonumber
\begin{split}
(S_{3})&=\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}M_{r}\textbf{x}_{i}=\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\textbf{x}_{i}\frac{1}{\beta}\sum_{j=1}^{n}(\delta_{j,r}-\alpha_{j}^{r})\textbf{x}_{j}=\\
&=\frac{1}{\beta}\sum_{i=1}^{n}\sum_{j=1}^{n}<\textbf{x}_{i},\textbf{x}_{j}>\sum_{r=1}^{k}\alpha_{i}^{r}(\delta_{i,r}-\alpha_{i}^{r}).\\
\end{split}
\end{equation}

Ya obtenidas las expresiones de $(S_{1}),(S_{2}),(S_{3})$ las sustituimos en \eqref{lagrangedesarrolloCS} 
\begin{equation}\label{lagrangedesarrolloCS2}\nonumber
\begin{split}
L(M_{r},\boldsymbol{\xi},\boldsymbol{\alpha})&=\frac{1}{2\beta}\sum_{i=1}^{n}\sum_{j=1}^{n}<\textbf{x}_{i},\textbf{x}_{j}>\sum_{r=1}^{k}(\delta_{y_{i},r}-\alpha_{i}^{r})(\delta_{y_{j},r}-\alpha_{j}^{r})-\\
&\hspace{5mm}-\frac{1}{\beta}\sum_{i=1}^{n}\sum_{j=1}^{n}<\textbf{x}_{i},\textbf{x}_{j}>\sum_{r=1}^{k}\delta_{y_{i},r}(\delta_{y_{j},r}-\alpha_{y_{j}}^{r})-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\delta_{y_{i},r}+n+\\
&\hspace{5mm}+\frac{1}{\beta}\sum_{i=1}^{n}\sum_{j=1}^{n}<\textbf{x}_{i},\textbf{x}_{j}>\sum_{r=1}^{k}\alpha_{i}^{r}(\delta_{i,r}-\alpha_{i}^{r})\\
&=\frac{1}{2\beta}\sum_{i=1}^{n}\sum_{j=1}^{n}<\textbf{x}_{i},\textbf{x}_{j}>\sum_{r=1}^{k}(\delta_{y_{i},r}-\alpha_{i}^{r})(\delta_{y_{j},r}-\alpha_{j}^{r})-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\delta_{y_{i},r}+n-\\
&\hspace{5mm}-\frac{1}{\beta}\sum_{i=1}^{n}\sum_{j=1}^{n}<\textbf{x}_{i},\textbf{x}_{j}>\sum_{r=1}^{k}(\delta_{y_{i},r}-\alpha_{i}^{r})(\delta_{y_{j},r}-\alpha_{j}^{r})=\\
&=-\frac{1}{2\beta}\sum_{i=1}^{n}\sum_{j=1}^{n}<\textbf{x}_{i},\textbf{x}_{j}>\sum_{r=1}^{k}(\delta_{y_{i},r}-\alpha_{i}^{r})(\delta_{y_{j},r}-\alpha_{j}^{r})-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\delta_{y_{i},r}+n.
\end{split}
\end{equation}

%\textbf{COMENTARIO: DESARROLLAR LO DE LA MULTIPLICACIÓN DE LAS DIFERENCIAS MÁS ABAJO. MÁS BIEN PREGUNTAR SI ES NECESARIO, PORQUE SI SE DESARROLLÁ SE VE.}

En el problema a resolver hemos excluido el término $n$ de la ecuación anterior por ser una constante y no influir en la solución del problema, luego el problema pasaría a ser el siguiente 

%\textbf{COMENTARIO: NO ES QUE NO AFECTE, SINO QUE NO AFECTA TANTO NO???}
 
 \begin{alignat*}{3}
\textbf{(PCS)} \quad & \min  \quad && -\frac{1}{2\beta}\sum_{i=1}^{n}\sum_{j=1}^{n}<\textbf{x}_{i},\textbf{x}_{j}>\sum_{r=1}^{k}(\delta_{y_{i},r}-\alpha_{i}^{r})(\delta_{y_{j},r}-\alpha_{j}^{r})-\sum_{i=1}^{n}\sum_{r=1}^{k}\alpha_{i}^{r}\delta_{y_{i},r}\qquad&& \\
& \text{s.a:}   \quad &&\sum_{r=1}^{k}\alpha_{i}^{r}=1, \quad && \\
&\qquad && \alpha_{i}^{r}\geq0,\quad i=1,\dots,n, \quad r\in\{1,\dots,k\}.
\end{alignat*}

\newpage
Hemos analizado 5 métodos diferentes para el modelo de SVM-Multiclase, a continuación se hará una recopilación de las propiedades de cada método, con lo que se podrán ver con más facilidad similitudes y diferencias entre estos 
\begin{table}[H]
	\begin{center}
		\begin{tabular}{ | m{2cm} | m{2.5cm} | m{2.5cm} | m{2.5cm} | m{2cm} | }
			\hline Métodos & Número de Clasficadores a entrenar& Forma de abordar aprendizaje & Forma de abordar test & Término independiente hiperplano  \\ \hline
			OVO (One vs One) & \centering$\frac{k(k-1)}{2}$ & Usando múltiples clasificadores binarios & Max-Wins & \hspace{9mm}Si\\ \hline
			OVA (One vs All)&\centering $k$ & Usando múltiples clasificadores binarios & Se selecciona la
			clase con el voto más alto & \hspace{9mm}Si\\ \hline
		 	DAGSVM  (Directed Acyclil Graph)&\centering $\frac{k(k-1)}{2}$ & Usando múltiples clasificadores binarios & Similar al método OVO & \hspace{9mm}Si\\ \hline
			W-W (Weston-Watkins)&\centering 1 & Resolviendo un solo problema de optimización & Usa el clasificador&\hspace{8mm} Si\\ \hline
			C-S (Crammer-Singer)&\centering 1 & Resolviendo un solo problema de optimización & Usa el clasificador& \hspace{8mm}No\\ \hline
		\end{tabular}
	\end{center}
\end{table}


% ----------------------------------------------------------------------

